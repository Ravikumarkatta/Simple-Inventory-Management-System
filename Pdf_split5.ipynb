{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravikumarkatta/Simple-Inventory-Management-System/blob/main/Pdf_split5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb1I1RptmEiu",
        "outputId": "bd2d3685-b482-4a6d-d709-d2c1c4cd614c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful!\n",
            "‚úÖ Vertical Line Detector loaded!\n",
            "üë§ User: __main__\n",
            "üïê Current time: 2025-08-08 12:13:25 UTC\n",
            "\n",
            "üìè Specialized for PDFs with vertical divider lines!\n",
            "\n",
            "üéØ Available functions:\n",
            "‚Ä¢ test_line_detection() - Analyze your PDF's vertical lines\n",
            "‚Ä¢ main_line_based_split() - Split using detected lines\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üéØ VERTICAL LINE DETECTOR - Perfect split using actual PDF lines\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    from tqdm.auto import tqdm\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    print(\"‚úÖ All imports successful!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    raise\n",
        "\n",
        "class VerticalLineDetector:\n",
        "    \"\"\"Detects vertical lines in PDF for perfect split positioning\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.debug_mode = True\n",
        "        print(\"üìè Vertical Line Detector initialized!\")\n",
        "\n",
        "    def detect_vertical_lines(self, pdf_path: str, sample_pages: int = 10) -> Dict:\n",
        "        \"\"\"\n",
        "        Detect vertical lines in PDF pages to find exact split positions\n",
        "\n",
        "        Returns:\n",
        "        - detected_lines: List of vertical line positions\n",
        "        - optimal_split: Best split position based on detected lines\n",
        "        - confidence: How confident we are in the detection\n",
        "        - line_consistency: How consistent lines are across pages\n",
        "        \"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            total_pages = len(doc)\n",
        "            sample_pages = min(sample_pages, total_pages)\n",
        "\n",
        "            print(f\"üìè Analyzing {sample_pages} pages for vertical lines...\")\n",
        "\n",
        "            all_vertical_lines = []\n",
        "            page_analyses = []\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Method 1: Drawing/Path Detection\n",
        "                lines_from_paths = self._detect_lines_from_paths(page)\n",
        "\n",
        "                # Method 2: Vector Graphics Detection\n",
        "                lines_from_vectors = self._detect_lines_from_vectors(page)\n",
        "\n",
        "                # Method 3: Text-based Line Detection (like \"|\" characters)\n",
        "                lines_from_text = self._detect_lines_from_text(page)\n",
        "\n",
        "                # Combine all detected lines for this page\n",
        "                page_lines = []\n",
        "                page_lines.extend(lines_from_paths)\n",
        "                page_lines.extend(lines_from_vectors)\n",
        "                page_lines.extend(lines_from_text)\n",
        "\n",
        "                # Remove duplicates and sort\n",
        "                page_lines = list(set(page_lines))\n",
        "                page_lines.sort()\n",
        "\n",
        "                page_analysis = {\n",
        "                    'page_num': page_num,\n",
        "                    'lines': page_lines,\n",
        "                    'page_width': page.rect.width,\n",
        "                    'page_height': page.rect.height,\n",
        "                    'methods': {\n",
        "                        'paths': len(lines_from_paths),\n",
        "                        'vectors': len(lines_from_vectors),\n",
        "                        'text': len(lines_from_text)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                page_analyses.append(page_analysis)\n",
        "                all_vertical_lines.extend(page_lines)\n",
        "\n",
        "                if self.debug_mode:\n",
        "                    print(f\"üìÑ Page {page_num + 1}: Found {len(page_lines)} vertical lines\")\n",
        "                    if page_lines:\n",
        "                        ratios = [line / page.rect.width for line in page_lines]\n",
        "                        print(f\"   ‚Ä¢ Line positions: {[f'{r:.1%}' for r in ratios]}\")\n",
        "\n",
        "            # Analyze all detected lines\n",
        "            final_analysis = self._analyze_detected_lines(all_vertical_lines, page_analyses)\n",
        "\n",
        "            return final_analysis\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _detect_lines_from_paths(self, page) -> List[float]:\n",
        "        \"\"\"Detect vertical lines from PDF drawing paths\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            # Get page drawings/paths\n",
        "            drawings = page.get_drawings()\n",
        "\n",
        "            for drawing in drawings:\n",
        "                items = drawing.get(\"items\", [])\n",
        "\n",
        "                for item in items:\n",
        "                    if item[0] == \"l\":  # Line drawing command\n",
        "                        # item format: (\"l\", point1, point2)\n",
        "                        if len(item) >= 3:\n",
        "                            p1, p2 = item[1], item[2]\n",
        "\n",
        "                            # Check if it's a vertical line (same x-coordinate)\n",
        "                            if abs(p1.x - p2.x) < 2:  # Allow 2-pixel tolerance\n",
        "                                # Check if it's long enough to be a page divider\n",
        "                                line_length = abs(p2.y - p1.y)\n",
        "                                if line_length > page.rect.height * 0.5:  # At least 50% of page height\n",
        "                                    vertical_lines.append(p1.x)\n",
        "\n",
        "                    elif item[0] == \"re\":  # Rectangle (might be a thin vertical rectangle)\n",
        "                        if len(item) >= 2:\n",
        "                            rect = item[1]\n",
        "                            # Check for thin vertical rectangles\n",
        "                            if rect.width <= 3 and rect.height > page.rect.height * 0.3:\n",
        "                                vertical_lines.append(rect.x0 + rect.width / 2)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Path detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_vectors(self, page) -> List[float]:\n",
        "        \"\"\"Detect lines from vector graphics elements\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            # Convert page to SVG to analyze vector elements\n",
        "            svg_text = page.get_svg_image()\n",
        "\n",
        "            # Simple SVG line detection\n",
        "            import re\n",
        "\n",
        "            # Look for vertical line patterns in SVG\n",
        "            # Pattern: <line x1=\"x\" y1=\"y1\" x2=\"x\" y2=\"y2\" ... /> where x1 ‚âà x2\n",
        "            line_pattern = r'<line[^>]*x1=\"([^\"]*)\"[^>]*y1=\"([^\"]*)\"[^>]*x2=\"([^\"]*)\"[^>]*y2=\"([^\"]*)\"[^>]*/?>'\n",
        "\n",
        "            matches = re.findall(line_pattern, svg_text)\n",
        "\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    x1, y1, x2, y2 = map(float, match)\n",
        "\n",
        "                    # Check if it's vertical (x1 ‚âà x2)\n",
        "                    if abs(x1 - x2) < 2:\n",
        "                        # Check if it's long enough\n",
        "                        line_length = abs(y2 - y1)\n",
        "                        if line_length > page.rect.height * 0.3:\n",
        "                            vertical_lines.append(x1)\n",
        "\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            # Also look for <rect> elements that might be vertical lines\n",
        "            rect_pattern = r'<rect[^>]*x=\"([^\"]*)\"[^>]*y=\"([^\"]*)\"[^>]*width=\"([^\"]*)\"[^>]*height=\"([^\"]*)\"[^>]*/?>'\n",
        "\n",
        "            rect_matches = re.findall(rect_pattern, svg_text)\n",
        "\n",
        "            for match in rect_matches:\n",
        "                try:\n",
        "                    x, y, width, height = map(float, match)\n",
        "\n",
        "                    # Check for thin vertical rectangles\n",
        "                    if width <= 3 and height > page.rect.height * 0.3:\n",
        "                        vertical_lines.append(x + width / 2)\n",
        "\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Vector detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_text(self, page) -> List[float]:\n",
        "        \"\"\"Detect vertical lines from text characters like |, «Ä, ‚èê\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            # Get text with positions\n",
        "            text_dict = page.get_text(\"dict\")\n",
        "\n",
        "            # Characters that might represent vertical lines\n",
        "            line_chars = ['|', '‚îÉ', '‚îã', '‚îá', '‚îÜ', '‚îÇ', '‚ïë', '«Ä', '‚èê', '«Å']\n",
        "\n",
        "            for block in text_dict.get(\"blocks\", []):\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line.get(\"spans\", []):\n",
        "                            text = span.get(\"text\", \"\")\n",
        "                            bbox = span.get(\"bbox\", [0, 0, 0, 0])\n",
        "\n",
        "                            # Check if this span contains line characters\n",
        "                            for char in line_chars:\n",
        "                                if char in text:\n",
        "                                    # Check if it's positioned like a page divider\n",
        "                                    char_x = (bbox[0] + bbox[2]) / 2\n",
        "                                    char_y_span = bbox[3] - bbox[1]\n",
        "\n",
        "                                    # If the character spans a good portion of the page height\n",
        "                                    if char_y_span > 20 or text.count(char) > 5:\n",
        "                                        vertical_lines.append(char_x)\n",
        "\n",
        "            # Also look for repeated pipe characters in a vertical column\n",
        "            blocks = page.get_text(\"dict\").get(\"blocks\", [])\n",
        "\n",
        "            # Group text by similar x-coordinates\n",
        "            x_groups = {}\n",
        "\n",
        "            for block in blocks:\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line.get(\"spans\", []):\n",
        "                            text = span.get(\"text\", \"\").strip()\n",
        "                            if any(char in text for char in line_chars):\n",
        "                                bbox = span.get(\"bbox\", [0, 0, 0, 0])\n",
        "                                x_pos = round((bbox[0] + bbox[2]) / 2)\n",
        "\n",
        "                                if x_pos not in x_groups:\n",
        "                                    x_groups[x_pos] = []\n",
        "                                x_groups[x_pos].append(text)\n",
        "\n",
        "            # Check for x-positions with multiple line characters\n",
        "            for x_pos, texts in x_groups.items():\n",
        "                line_char_count = sum(sum(text.count(char) for char in line_chars) for text in texts)\n",
        "                if line_char_count >= 3:  # At least 3 line characters in this column\n",
        "                    vertical_lines.append(float(x_pos))\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Text-based line detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _analyze_detected_lines(self, all_lines: List[float], page_analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze all detected lines to find the best split position\"\"\"\n",
        "\n",
        "        if not all_lines:\n",
        "            return {\n",
        "                'optimal_split': 0.5,\n",
        "                'confidence': 0.1,\n",
        "                'detected_lines': [],\n",
        "                'method': 'fallback',\n",
        "                'line_consistency': 0.0\n",
        "            }\n",
        "\n",
        "        # Get page width (use from first page)\n",
        "        page_width = page_analyses[0]['page_width'] if page_analyses else 595  # Default A4 width\n",
        "\n",
        "        # Convert to ratios\n",
        "        line_ratios = [line / page_width for line in all_lines if 0.1 <= line / page_width <= 0.9]\n",
        "\n",
        "        if not line_ratios:\n",
        "            return {\n",
        "                'optimal_split': 0.5,\n",
        "                'confidence': 0.1,\n",
        "                'detected_lines': [],\n",
        "                'method': 'fallback',\n",
        "                'line_consistency': 0.0\n",
        "            }\n",
        "\n",
        "        # Cluster similar line positions (within 5%)\n",
        "        clusters = []\n",
        "        line_ratios.sort()\n",
        "\n",
        "        for ratio in line_ratios:\n",
        "            # Find if this ratio belongs to an existing cluster\n",
        "            added_to_cluster = False\n",
        "            for cluster in clusters:\n",
        "                if any(abs(ratio - existing_ratio) <= 0.05 for existing_ratio in cluster):\n",
        "                    cluster.append(ratio)\n",
        "                    added_to_cluster = True\n",
        "                    break\n",
        "\n",
        "            if not added_to_cluster:\n",
        "                clusters.append([ratio])\n",
        "\n",
        "        # Find the most consistent cluster (most occurrences)\n",
        "        best_cluster = max(clusters, key=len)\n",
        "        optimal_split = sum(best_cluster) / len(best_cluster)  # Average of cluster\n",
        "\n",
        "        # Calculate confidence based on:\n",
        "        # 1. How many times this line appears\n",
        "        # 2. How consistent it is across pages\n",
        "        # 3. How many different detection methods found it\n",
        "\n",
        "        line_frequency = len(best_cluster)\n",
        "        total_pages = len(page_analyses)\n",
        "        frequency_confidence = min(1.0, line_frequency / total_pages)\n",
        "\n",
        "        # Check consistency across detection methods\n",
        "        method_count = 0\n",
        "        for page_analysis in page_analyses:\n",
        "            methods = page_analysis['methods']\n",
        "            if any(methods.values()):\n",
        "                method_count += 1\n",
        "\n",
        "        method_confidence = min(1.0, method_count / total_pages)\n",
        "\n",
        "        # Calculate how tight the cluster is (consistency)\n",
        "        if len(best_cluster) > 1:\n",
        "            cluster_tightness = 1 - (max(best_cluster) - min(best_cluster))\n",
        "        else:\n",
        "            cluster_tightness = 1.0\n",
        "\n",
        "        final_confidence = (frequency_confidence + method_confidence + cluster_tightness) / 3\n",
        "\n",
        "        return {\n",
        "            'optimal_split': optimal_split,\n",
        "            'confidence': final_confidence,\n",
        "            'detected_lines': all_lines,\n",
        "            'line_ratios': line_ratios,\n",
        "            'clusters': clusters,\n",
        "            'best_cluster': best_cluster,\n",
        "            'method': 'vertical_line_detection',\n",
        "            'line_consistency': cluster_tightness,\n",
        "            'frequency': line_frequency,\n",
        "            'pages_analyzed': total_pages,\n",
        "            'detection_summary': {\n",
        "                'total_lines_found': len(all_lines),\n",
        "                'valid_ratios': len(line_ratios),\n",
        "                'clusters_found': len(clusters),\n",
        "                'best_cluster_size': len(best_cluster)\n",
        "            }\n",
        "        }\n",
        "\n",
        "class LineBasedPDFSplitter:\n",
        "    \"\"\"PDF Splitter using vertical line detection\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.detector = VerticalLineDetector()\n",
        "        print(\"üìè Line-Based PDF Splitter initialized!\")\n",
        "\n",
        "    def split_using_detected_lines(self, input_path: str, output_path: str) -> bool:\n",
        "        \"\"\"Split PDF using detected vertical lines\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Detect vertical lines\n",
        "            print(\"üìè Step 1: Detecting vertical lines...\")\n",
        "            line_analysis = self.detector.detect_vertical_lines(input_path)\n",
        "\n",
        "            optimal_ratio = line_analysis['optimal_split']\n",
        "            confidence = line_analysis['confidence']\n",
        "\n",
        "            print(f\"\\nüìä Line Detection Results:\")\n",
        "            print(f\"   ‚Ä¢ Optimal split position: {optimal_ratio:.1%}\")\n",
        "            print(f\"   ‚Ä¢ Detection confidence: {confidence:.1%}\")\n",
        "            print(f\"   ‚Ä¢ Line consistency: {line_analysis['line_consistency']:.1%}\")\n",
        "            print(f\"   ‚Ä¢ Total lines found: {line_analysis['detection_summary']['total_lines_found']}\")\n",
        "            print(f\"   ‚Ä¢ Best cluster size: {line_analysis['detection_summary']['best_cluster_size']}\")\n",
        "\n",
        "            if confidence < 0.3:\n",
        "                print(\"‚ö†Ô∏è Low confidence in line detection. Results may not be optimal.\")\n",
        "            elif confidence > 0.8:\n",
        "                print(\"üéØ High confidence! Line detection is very reliable.\")\n",
        "\n",
        "            # Step 2: Apply the split\n",
        "            print(f\"\\n‚úÇÔ∏è Step 2: Splitting at {optimal_ratio:.1%}...\")\n",
        "\n",
        "            input_doc = fitz.open(input_path)\n",
        "            output_doc = fitz.open()\n",
        "\n",
        "            page_count = len(input_doc)\n",
        "            success_count = 0\n",
        "\n",
        "            with tqdm(total=page_count, desc=\"üìÑ Line-Based Splitting\") as pbar:\n",
        "                for page_num in range(page_count):\n",
        "                    try:\n",
        "                        page = input_doc[page_num]\n",
        "                        rect = page.rect\n",
        "\n",
        "                        # Apply the detected split position\n",
        "                        split_pos = rect.width * optimal_ratio\n",
        "\n",
        "                        # Left half\n",
        "                        left_clip = fitz.Rect(0, 0, split_pos, rect.height)\n",
        "                        left_page = output_doc.new_page(width=split_pos, height=rect.height)\n",
        "                        left_page.show_pdf_page(fitz.Rect(0, 0, split_pos, rect.height),\n",
        "                                               input_doc, page_num, clip=left_clip)\n",
        "\n",
        "                        # Right half\n",
        "                        right_clip = fitz.Rect(split_pos, 0, rect.width, rect.height)\n",
        "                        right_page = output_doc.new_page(width=rect.width - split_pos, height=rect.height)\n",
        "                        right_page.show_pdf_page(fitz.Rect(0, 0, rect.width - split_pos, rect.height),\n",
        "                                                input_doc, page_num, clip=right_clip)\n",
        "\n",
        "                        success_count += 1\n",
        "                        pbar.update(1)\n",
        "\n",
        "                    except Exception as page_error:\n",
        "                        print(f\"‚ö†Ô∏è Page {page_num + 1} error: {page_error}\")\n",
        "                        continue\n",
        "\n",
        "            # Step 3: Save\n",
        "            print(\"üíæ Step 3: Saving line-based split...\")\n",
        "\n",
        "            try:\n",
        "                output_doc.save(output_path, garbage=4, deflate=True)\n",
        "\n",
        "                # Results\n",
        "                output_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "                created_pages = len(output_doc)\n",
        "\n",
        "                print(f\"\\nüéâ Line-Based Split Complete!\")\n",
        "                print(f\"üìÑ Created: {created_pages} pages from {page_count} original\")\n",
        "                print(f\"üìÅ Size: {output_size:.2f} MB\")\n",
        "                print(f\"üéØ Success rate: {success_count/page_count:.1%}\")\n",
        "                print(f\"üìè Split position: {optimal_ratio:.1%} (based on detected vertical lines)\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            except Exception as save_error:\n",
        "                print(f\"‚ùå Save error: {save_error}\")\n",
        "                return False\n",
        "\n",
        "            finally:\n",
        "                input_doc.close()\n",
        "                output_doc.close()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            return False\n",
        "\n",
        "def test_line_detection():\n",
        "    \"\"\"Test vertical line detection on your PDF\"\"\"\n",
        "\n",
        "    print(\"üìè VERTICAL LINE DETECTION TEST\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find PDF\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith('SMART_')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"üìÅ Testing: {pdf_file}\")\n",
        "\n",
        "    # Create detector\n",
        "    detector = VerticalLineDetector()\n",
        "\n",
        "    # Detect lines\n",
        "    analysis = detector.detect_vertical_lines(pdf_file, sample_pages=10)\n",
        "\n",
        "    # Show detailed results\n",
        "    print(f\"\\nüìä DETAILED RESULTS:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Optimal split position: {analysis['optimal_split']:.1%}\")\n",
        "    print(f\"Detection confidence: {analysis['confidence']:.1%}\")\n",
        "    print(f\"Line consistency: {analysis['line_consistency']:.1%}\")\n",
        "    print(f\"Total lines detected: {len(analysis['detected_lines'])}\")\n",
        "\n",
        "    if analysis['line_ratios']:\n",
        "        print(f\"\\nAll detected line positions:\")\n",
        "        for i, ratio in enumerate(analysis['line_ratios']):\n",
        "            print(f\"  Line {i+1}: {ratio:.1%}\")\n",
        "\n",
        "    if analysis['clusters']:\n",
        "        print(f\"\\nLine clusters found:\")\n",
        "        for i, cluster in enumerate(analysis['clusters']):\n",
        "            avg_pos = sum(cluster) / len(cluster)\n",
        "            print(f\"  Cluster {i+1}: {avg_pos:.1%} (appears {len(cluster)} times)\")\n",
        "\n",
        "    print(f\"\\nüéØ RECOMMENDATION:\")\n",
        "    print(f\"Split at {analysis['optimal_split']:.1%} with {analysis['confidence']:.1%} confidence\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def main_line_based_split():\n",
        "    \"\"\"Main function using line detection\"\"\"\n",
        "\n",
        "    print(\"üìè LINE-BASED PDF SPLITTER\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"üéØ Perfect for PDFs with vertical divider lines\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Find PDF\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith('SMART_')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "\n",
        "    # Create splitter\n",
        "    splitter = LineBasedPDFSplitter()\n",
        "\n",
        "    # Generate output name\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    output_file = f\"LINE_SPLIT_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "    # Split using line detection\n",
        "    if splitter.split_using_detected_lines(pdf_file, output_file):\n",
        "        try:\n",
        "            files.download(output_file)\n",
        "            print(\"‚úÖ Download successful!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download failed: {e}\")\n",
        "\n",
        "print(\"‚úÖ Vertical Line Detector loaded!\")\n",
        "print(f\"üë§ User: {__name__ if '__name__' in globals() else 'Ravi-katta-dev'}\")\n",
        "print(f\"üïê Current time: 2025-08-08 12:13:25 UTC\")\n",
        "print(\"\\nüìè Specialized for PDFs with vertical divider lines!\")\n",
        "print(\"\\nüéØ Available functions:\")\n",
        "print(\"‚Ä¢ test_line_detection() - Analyze your PDF's vertical lines\")\n",
        "print(\"‚Ä¢ main_line_based_split() - Split using detected lines\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GclEIcwNI06o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# üéØ COMPLETE INTELLIGENT PDF SPLITTER - Full Featured Solution\n",
        "# =============================================================================\n",
        "# Author: Advanced PDF Processing System\n",
        "# Date: 2025-08-08 12:18:45 UTC\n",
        "# User: Ravi-katta-dev\n",
        "# Version: 4.0 - Complete Solution\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import zipfile\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Dict, Any\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    from tqdm.auto import tqdm\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    print(\"‚úÖ All imports successful!\")\n",
        "    print(f\"üïê Session started: 2025-08-08 12:18:45 UTC\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Please install required packages:\")\n",
        "    print(\"!pip install PyMuPDF tqdm ipywidgets\")\n",
        "    raise\n",
        "\n",
        "class CompletePDFSplitter:\n",
        "    \"\"\"Complete PDF Splitter with all advanced features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.processed_files = []\n",
        "        self.debug_mode = True\n",
        "        self.session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        print(f\"üéØ Complete PDF Splitter v4.0 initialized!\")\n",
        "        print(f\"üì± Session ID: {self.session_id}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # üìè VERTICAL LINE DETECTION SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def detect_vertical_lines(self, pdf_path: str, sample_pages: int = 10) -> Dict:\n",
        "        \"\"\"Detect vertical lines in PDF for exact split positioning\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            total_pages = len(doc)\n",
        "            sample_pages = min(sample_pages, total_pages)\n",
        "\n",
        "            print(f\"üìè Analyzing {sample_pages} pages for vertical lines...\")\n",
        "\n",
        "            all_vertical_lines = []\n",
        "            page_analyses = []\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Multi-method line detection\n",
        "                lines_from_paths = self._detect_lines_from_paths(page)\n",
        "                lines_from_vectors = self._detect_lines_from_vectors(page)\n",
        "                lines_from_text = self._detect_lines_from_text(page)\n",
        "                lines_from_images = self._detect_lines_from_images(page)\n",
        "\n",
        "                # Combine all methods\n",
        "                page_lines = []\n",
        "                page_lines.extend(lines_from_paths)\n",
        "                page_lines.extend(lines_from_vectors)\n",
        "                page_lines.extend(lines_from_text)\n",
        "                page_lines.extend(lines_from_images)\n",
        "\n",
        "                # Remove duplicates and filter\n",
        "                page_lines = self._filter_and_clean_lines(page_lines, page.rect.width)\n",
        "\n",
        "                page_analysis = {\n",
        "                    'page_num': page_num + 1,\n",
        "                    'lines': page_lines,\n",
        "                    'page_width': page.rect.width,\n",
        "                    'page_height': page.rect.height,\n",
        "                    'methods_count': {\n",
        "                        'paths': len(lines_from_paths),\n",
        "                        'vectors': len(lines_from_vectors),\n",
        "                        'text': len(lines_from_text),\n",
        "                        'images': len(lines_from_images)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                page_analyses.append(page_analysis)\n",
        "                all_vertical_lines.extend(page_lines)\n",
        "\n",
        "                if self.debug_mode:\n",
        "                    print(f\"üìÑ Page {page_num + 1}: Found {len(page_lines)} vertical lines\")\n",
        "                    if page_lines:\n",
        "                        ratios = [line / page.rect.width for line in page_lines]\n",
        "                        print(f\"   ‚Ä¢ Positions: {[f'{r:.1%}' for r in ratios[:5]]}\")\n",
        "\n",
        "            # Analyze all detected lines\n",
        "            final_analysis = self._analyze_all_detected_lines(all_vertical_lines, page_analyses)\n",
        "\n",
        "            return final_analysis\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _detect_lines_from_paths(self, page) -> List[float]:\n",
        "        \"\"\"Method 1: Detect lines from PDF drawing paths\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            drawings = page.get_drawings()\n",
        "\n",
        "            for drawing in drawings:\n",
        "                items = drawing.get(\"items\", [])\n",
        "\n",
        "                for item in items:\n",
        "                    if item[0] == \"l\":  # Line command\n",
        "                        if len(item) >= 3:\n",
        "                            p1, p2 = item[1], item[2]\n",
        "\n",
        "                            # Check for vertical line\n",
        "                            if abs(p1.x - p2.x) < 3:  # Tolerance for vertical\n",
        "                                line_length = abs(p2.y - p1.y)\n",
        "                                if line_length > page.rect.height * 0.3:\n",
        "                                    vertical_lines.append(p1.x)\n",
        "\n",
        "                    elif item[0] == \"re\":  # Rectangle (thin vertical)\n",
        "                        if len(item) >= 2:\n",
        "                            rect = item[1]\n",
        "                            if rect.width <= 5 and rect.height > page.rect.height * 0.2:\n",
        "                                vertical_lines.append(rect.x0 + rect.width / 2)\n",
        "\n",
        "                    elif item[0] == \"c\":  # Curve (might be decorative line)\n",
        "                        if len(item) >= 4:\n",
        "                            p1, p2, p3, p4 = item[1], item[2], item[3], item[4]\n",
        "                            # Check if curve is essentially vertical\n",
        "                            x_variance = max(p1.x, p2.x, p3.x, p4.x) - min(p1.x, p2.x, p3.x, p4.x)\n",
        "                            if x_variance < 5:\n",
        "                                avg_x = (p1.x + p2.x + p3.x + p4.x) / 4\n",
        "                                vertical_lines.append(avg_x)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Path detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_vectors(self, page) -> List[float]:\n",
        "        \"\"\"Method 2: Detect lines from SVG vector graphics\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            svg_text = page.get_svg_image()\n",
        "\n",
        "            # Enhanced SVG pattern matching\n",
        "            patterns = [\n",
        "                r'<line[^>]*x1=\"([^\"]*)\"[^>]*y1=\"([^\"]*)\"[^>]*x2=\"([^\"]*)\"[^>]*y2=\"([^\"]*)\"[^>]*/?>',\n",
        "                r'<rect[^>]*x=\"([^\"]*)\"[^>]*y=\"([^\"]*)\"[^>]*width=\"([^\"]*)\"[^>]*height=\"([^\"]*)\"[^>]*/?>',\n",
        "                r'<path[^>]*d=\"M\\s*([^,\\s]+)[,\\s]+([^,\\s]+)\\s*L\\s*([^,\\s]+)[,\\s]+([^,\\s]+)\"[^>]*/?>',\n",
        "            ]\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, svg_text)\n",
        "\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        if len(match) == 4:\n",
        "                            if 'line' in pattern or 'path' in pattern:\n",
        "                                x1, y1, x2, y2 = map(float, match)\n",
        "                                if abs(x1 - x2) < 3:  # Vertical line\n",
        "                                    line_length = abs(y2 - y1)\n",
        "                                    if line_length > page.rect.height * 0.2:\n",
        "                                        vertical_lines.append(x1)\n",
        "\n",
        "                            elif 'rect' in pattern:\n",
        "                                x, y, width, height = map(float, match)\n",
        "                                if width <= 5 and height > page.rect.height * 0.2:\n",
        "                                    vertical_lines.append(x + width / 2)\n",
        "\n",
        "                    except (ValueError, IndexError):\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Vector detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_text(self, page) -> List[float]:\n",
        "        \"\"\"Method 3: Detect lines from text characters\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            text_dict = page.get_text(\"dict\")\n",
        "\n",
        "            # Extended line characters\n",
        "            line_chars = ['|', '‚îÉ', '‚îã', '‚îá', '‚îÜ', '‚îÇ', '‚ïë', '«Ä', '‚èê', '«Å', 'Ô∏±', '‰∏®', 'ÔΩú']\n",
        "\n",
        "            # Collect all text spans with line characters\n",
        "            line_positions = {}\n",
        "\n",
        "            for block in text_dict.get(\"blocks\", []):\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        for span in line.get(\"spans\", []):\n",
        "                            text = span.get(\"text\", \"\")\n",
        "                            bbox = span.get(\"bbox\", [0, 0, 0, 0])\n",
        "\n",
        "                            for char in line_chars:\n",
        "                                if char in text:\n",
        "                                    char_x = round((bbox[0] + bbox[2]) / 2)\n",
        "\n",
        "                                    if char_x not in line_positions:\n",
        "                                        line_positions[char_x] = 0\n",
        "                                    line_positions[char_x] += text.count(char)\n",
        "\n",
        "            # Filter positions with enough line characters\n",
        "            for x_pos, count in line_positions.items():\n",
        "                if count >= 3:  # At least 3 line characters\n",
        "                    vertical_lines.append(float(x_pos))\n",
        "\n",
        "            # Also detect repeated patterns that might indicate borders\n",
        "            full_text = page.get_text()\n",
        "\n",
        "            # Look for repeated sequences that might be borders\n",
        "            border_patterns = ['|', '-|', '|-', '||', '‚îÇ', '‚îÄ‚îÄ']\n",
        "\n",
        "            for pattern in border_patterns:\n",
        "                if full_text.count(pattern) > 5:  # Pattern appears multiple times\n",
        "                    # This suggests there might be a structured layout\n",
        "                    # Add common positions for such patterns\n",
        "                    page_width = page.rect.width\n",
        "                    common_ratios = [0.25, 0.33, 0.4, 0.5, 0.6, 0.67, 0.75]\n",
        "\n",
        "                    for ratio in common_ratios:\n",
        "                        if any(abs(line/page_width - ratio) < 0.05 for line in vertical_lines):\n",
        "                            # Only add if we already detected something near this ratio\n",
        "                            vertical_lines.append(page_width * ratio)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Text line detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _detect_lines_from_images(self, page) -> List[float]:\n",
        "        \"\"\"Method 4: Detect lines from embedded images\"\"\"\n",
        "        vertical_lines = []\n",
        "\n",
        "        try:\n",
        "            # Get images on the page\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            for img in image_list:\n",
        "                try:\n",
        "                    bbox = page.get_image_bbox(img)\n",
        "                    if bbox:\n",
        "                        # Check if image is thin and vertical (might be a line)\n",
        "                        if bbox.width <= 10 and bbox.height > page.rect.height * 0.2:\n",
        "                            vertical_lines.append(bbox.x0 + bbox.width / 2)\n",
        "\n",
        "                        # Check for images that might contain line graphics\n",
        "                        if bbox.width > 20 and bbox.height > page.rect.height * 0.5:\n",
        "                            # This might be a large image with embedded lines\n",
        "                            # Add common split positions within the image\n",
        "                            img_center = bbox.x0 + bbox.width / 2\n",
        "\n",
        "                            # Check if image spans a significant portion of the page\n",
        "                            if bbox.width > page.rect.width * 0.3:\n",
        "                                vertical_lines.append(img_center)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.debug_mode:\n",
        "                print(f\"   ‚ö†Ô∏è Image line detection error: {e}\")\n",
        "\n",
        "        return vertical_lines\n",
        "\n",
        "    def _filter_and_clean_lines(self, lines: List[float], page_width: float) -> List[float]:\n",
        "        \"\"\"Filter and clean detected lines\"\"\"\n",
        "        if not lines:\n",
        "            return []\n",
        "\n",
        "        # Remove duplicates with tolerance\n",
        "        cleaned_lines = []\n",
        "        lines.sort()\n",
        "\n",
        "        for line in lines:\n",
        "            # Check if this line is valid (within page bounds and reasonable position)\n",
        "            if 0.1 * page_width <= line <= 0.9 * page_width:\n",
        "                # Check if it's too close to an existing line\n",
        "                if not any(abs(line - existing) < page_width * 0.02 for existing in cleaned_lines):\n",
        "                    cleaned_lines.append(line)\n",
        "\n",
        "        return cleaned_lines\n",
        "\n",
        "    def _analyze_all_detected_lines(self, all_lines: List[float], page_analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze all detected lines to find optimal split\"\"\"\n",
        "\n",
        "        if not all_lines or not page_analyses:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        page_width = page_analyses[0]['page_width']\n",
        "\n",
        "        # Convert to ratios and filter\n",
        "        line_ratios = []\n",
        "        for line in all_lines:\n",
        "            ratio = line / page_width\n",
        "            if 0.15 <= ratio <= 0.85:  # Reasonable split range\n",
        "                line_ratios.append(ratio)\n",
        "\n",
        "        if not line_ratios:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        # Advanced clustering with multiple methods\n",
        "        clusters = self._perform_advanced_clustering(line_ratios)\n",
        "\n",
        "        if not clusters:\n",
        "            return self._get_fallback_analysis()\n",
        "\n",
        "        # Select best cluster\n",
        "        best_cluster = self._select_best_cluster(clusters, page_analyses)\n",
        "\n",
        "        # Calculate final metrics\n",
        "        optimal_split = sum(best_cluster['ratios']) / len(best_cluster['ratios'])\n",
        "        confidence = self._calculate_confidence(best_cluster, clusters, page_analyses)\n",
        "        consistency = self._calculate_consistency(best_cluster)\n",
        "\n",
        "        return {\n",
        "            'optimal_split': optimal_split,\n",
        "            'confidence': confidence,\n",
        "            'line_consistency': consistency,\n",
        "            'detected_lines': all_lines,\n",
        "            'line_ratios': line_ratios,\n",
        "            'clusters': clusters,\n",
        "            'best_cluster': best_cluster,\n",
        "            'method': 'advanced_line_detection',\n",
        "            'pages_analyzed': len(page_analyses),\n",
        "            'detection_summary': {\n",
        "                'total_lines_found': len(all_lines),\n",
        "                'valid_ratios': len(line_ratios),\n",
        "                'clusters_found': len(clusters),\n",
        "                'best_cluster_size': len(best_cluster['ratios'])\n",
        "            },\n",
        "            'page_details': page_analyses\n",
        "        }\n",
        "\n",
        "    def _perform_advanced_clustering(self, ratios: List[float]) -> List[Dict]:\n",
        "        \"\"\"Perform advanced clustering of line positions\"\"\"\n",
        "        if not ratios:\n",
        "            return []\n",
        "\n",
        "        ratios_sorted = sorted(ratios)\n",
        "        clusters = []\n",
        "\n",
        "        # Dynamic tolerance based on data distribution\n",
        "        tolerance = max(0.02, (max(ratios) - min(ratios)) / 10)\n",
        "\n",
        "        for ratio in ratios_sorted:\n",
        "            added = False\n",
        "\n",
        "            for cluster in clusters:\n",
        "                cluster_center = sum(cluster['ratios']) / len(cluster['ratios'])\n",
        "                if abs(ratio - cluster_center) <= tolerance:\n",
        "                    cluster['ratios'].append(ratio)\n",
        "                    added = True\n",
        "                    break\n",
        "\n",
        "            if not added:\n",
        "                clusters.append({\n",
        "                    'ratios': [ratio],\n",
        "                    'center': ratio,\n",
        "                    'weight': 1\n",
        "                })\n",
        "\n",
        "        # Update cluster centers and weights\n",
        "        for cluster in clusters:\n",
        "            cluster['center'] = sum(cluster['ratios']) / len(cluster['ratios'])\n",
        "            cluster['weight'] = len(cluster['ratios'])\n",
        "            cluster['spread'] = max(cluster['ratios']) - min(cluster['ratios']) if len(cluster['ratios']) > 1 else 0\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def _select_best_cluster(self, clusters: List[Dict], page_analyses: List[Dict]) -> Dict:\n",
        "        \"\"\"Select the best cluster based on multiple criteria\"\"\"\n",
        "\n",
        "        def score_cluster(cluster):\n",
        "            # Weight (frequency)\n",
        "            weight_score = cluster['weight'] / len(page_analyses)\n",
        "\n",
        "            # Consistency (low spread)\n",
        "            spread_score = 1 - min(1, cluster['spread'] / 0.1)\n",
        "\n",
        "            # Position preference (avoid extreme edges)\n",
        "            center = cluster['center']\n",
        "            position_score = 1 - abs(center - 0.5) * 2  # Prefer center positions\n",
        "\n",
        "            # Combined score\n",
        "            return (weight_score * 0.5 + spread_score * 0.3 + position_score * 0.2)\n",
        "\n",
        "        return max(clusters, key=score_cluster)\n",
        "\n",
        "    def _calculate_confidence(self, best_cluster: Dict, all_clusters: List[Dict], page_analyses: List[Dict]) -> float:\n",
        "        \"\"\"Calculate confidence in the detection\"\"\"\n",
        "\n",
        "        # Frequency confidence\n",
        "        frequency_conf = min(1.0, best_cluster['weight'] / len(page_analyses))\n",
        "\n",
        "        # Consistency confidence\n",
        "        consistency_conf = 1 - min(1, best_cluster['spread'] / 0.05)\n",
        "\n",
        "        # Method diversity confidence\n",
        "        total_methods = sum(\n",
        "            sum(page['methods_count'].values())\n",
        "            for page in page_analyses\n",
        "        )\n",
        "        method_conf = min(1.0, total_methods / (len(page_analyses) * 2))\n",
        "\n",
        "        # Dominance confidence (how much better is this cluster than others)\n",
        "        if len(all_clusters) > 1:\n",
        "            other_weights = [c['weight'] for c in all_clusters if c != best_cluster]\n",
        "            dominance_conf = best_cluster['weight'] / (max(other_weights) + 1) if other_weights else 1.0\n",
        "            dominance_conf = min(1.0, dominance_conf)\n",
        "        else:\n",
        "            dominance_conf = 1.0\n",
        "\n",
        "        # Combined confidence\n",
        "        confidence = (frequency_conf * 0.3 + consistency_conf * 0.3 +\n",
        "                     method_conf * 0.2 + dominance_conf * 0.2)\n",
        "\n",
        "        return confidence\n",
        "\n",
        "    def _calculate_consistency(self, cluster: Dict) -> float:\n",
        "        \"\"\"Calculate consistency of the cluster\"\"\"\n",
        "        if len(cluster['ratios']) <= 1:\n",
        "            return 1.0\n",
        "\n",
        "        return 1 - min(1, cluster['spread'] / 0.1)\n",
        "\n",
        "    def _get_fallback_analysis(self) -> Dict:\n",
        "        \"\"\"Fallback analysis when line detection fails\"\"\"\n",
        "        return {\n",
        "            'optimal_split': 0.5,\n",
        "            'confidence': 0.1,\n",
        "            'line_consistency': 0.0,\n",
        "            'detected_lines': [],\n",
        "            'method': 'fallback',\n",
        "            'detection_summary': {\n",
        "                'total_lines_found': 0,\n",
        "                'valid_ratios': 0,\n",
        "                'clusters_found': 0,\n",
        "                'best_cluster_size': 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # =============================================================================\n",
        "    # üß† CONTENT ANALYSIS SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def analyze_content_layout(self, pdf_path: str, sample_pages: int = 5) -> Dict:\n",
        "        \"\"\"Comprehensive content analysis\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            sample_pages = min(sample_pages, len(doc))\n",
        "            print(f\"üß† Analyzing content layout from {sample_pages} pages...\")\n",
        "\n",
        "            content_analysis = {\n",
        "                'text_density_map': [],\n",
        "                'layout_patterns': [],\n",
        "                'font_analysis': [],\n",
        "                'image_positions': [],\n",
        "                'whitespace_analysis': []\n",
        "            }\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Analyze text density\n",
        "                density_analysis = self._analyze_text_density(page)\n",
        "                content_analysis['text_density_map'].append(density_analysis)\n",
        "\n",
        "                # Analyze layout patterns\n",
        "                layout_analysis = self._analyze_layout_patterns(page)\n",
        "                content_analysis['layout_patterns'].append(layout_analysis)\n",
        "\n",
        "                # Analyze fonts and formatting\n",
        "                font_analysis = self._analyze_fonts_and_formatting(page)\n",
        "                content_analysis['font_analysis'].append(font_analysis)\n",
        "\n",
        "                # Analyze images and graphics\n",
        "                image_analysis = self._analyze_images_and_graphics(page)\n",
        "                content_analysis['image_positions'].append(image_analysis)\n",
        "\n",
        "                # Analyze whitespace\n",
        "                whitespace_analysis = self._analyze_whitespace_distribution(page)\n",
        "                content_analysis['whitespace_analysis'].append(whitespace_analysis)\n",
        "\n",
        "            # Combine all analyses\n",
        "            final_content_analysis = self._combine_content_analyses(content_analysis)\n",
        "\n",
        "            return final_content_analysis\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _analyze_text_density(self, page) -> Dict:\n",
        "        \"\"\"Analyze text density across the page\"\"\"\n",
        "        rect = page.rect\n",
        "\n",
        "        # Create density grid\n",
        "        grid_cols, grid_rows = 20, 20\n",
        "        density_grid = [[0 for _ in range(grid_cols)] for _ in range(grid_rows)]\n",
        "\n",
        "        # Get text blocks\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                text_length = 0\n",
        "\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line.get(\"spans\", []):\n",
        "                        text_length += len(span.get(\"text\", \"\").strip())\n",
        "\n",
        "                if text_length > 0:\n",
        "                    # Map to grid\n",
        "                    col = min(int((bbox[0] + bbox[2]) / 2 / rect.width * grid_cols), grid_cols - 1)\n",
        "                    row = min(int((bbox[1] + bbox[3]) / 2 / rect.height * grid_rows), grid_rows - 1)\n",
        "\n",
        "                    density_grid[row][col] += text_length\n",
        "\n",
        "        # Analyze vertical distribution\n",
        "        vertical_density = [sum(density_grid[row][col] for row in range(grid_rows)) for col in range(grid_cols)]\n",
        "\n",
        "        # Find optimal vertical split\n",
        "        min_density_col = 0\n",
        "        min_density = float('inf')\n",
        "\n",
        "        for col in range(int(grid_cols * 0.2), int(grid_cols * 0.8)):\n",
        "            if vertical_density[col] < min_density:\n",
        "                min_density = vertical_density[col]\n",
        "                min_density_col = col\n",
        "\n",
        "        optimal_split = (min_density_col + 0.5) / grid_cols\n",
        "\n",
        "        return {\n",
        "            'density_grid': density_grid,\n",
        "            'vertical_density': vertical_density,\n",
        "            'optimal_split': optimal_split,\n",
        "            'total_text': sum(vertical_density)\n",
        "        }\n",
        "\n",
        "    def _analyze_layout_patterns(self, page) -> Dict:\n",
        "        \"\"\"Analyze layout patterns and structure\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        patterns = {\n",
        "            'columns': 0,\n",
        "            'alignment_patterns': {'left': 0, 'center': 0, 'right': 0},\n",
        "            'spacing_patterns': [],\n",
        "            'block_sizes': []\n",
        "        }\n",
        "\n",
        "        block_positions = []\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                block_width = bbox[2] - bbox[0]\n",
        "                block_height = bbox[3] - bbox[1]\n",
        "\n",
        "                patterns['block_sizes'].append({\n",
        "                    'width': block_width,\n",
        "                    'height': block_height,\n",
        "                    'x': bbox[0],\n",
        "                    'y': bbox[1]\n",
        "                })\n",
        "\n",
        "                block_positions.append(bbox[0])  # Left edge\n",
        "\n",
        "                # Analyze alignment\n",
        "                page_width = page.rect.width\n",
        "                if bbox[0] < page_width * 0.1:\n",
        "                    patterns['alignment_patterns']['left'] += 1\n",
        "                elif bbox[2] > page_width * 0.9:\n",
        "                    patterns['alignment_patterns']['right'] += 1\n",
        "                else:\n",
        "                    patterns['alignment_patterns']['center'] += 1\n",
        "\n",
        "        # Detect column structure\n",
        "        if block_positions:\n",
        "            block_positions.sort()\n",
        "            gaps = []\n",
        "\n",
        "            for i in range(1, len(block_positions)):\n",
        "                gap = block_positions[i] - block_positions[i-1]\n",
        "                if gap > 20:  # Significant gap\n",
        "                    gaps.append(gap)\n",
        "\n",
        "            patterns['spacing_patterns'] = gaps\n",
        "            patterns['columns'] = len(set(round(pos / 50) * 50 for pos in block_positions))\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _analyze_fonts_and_formatting(self, page) -> Dict:\n",
        "        \"\"\"Analyze fonts and text formatting\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "\n",
        "        font_analysis = {\n",
        "            'font_sizes': {},\n",
        "            'font_families': {},\n",
        "            'text_styles': {'bold': 0, 'italic': 0, 'normal': 0},\n",
        "            'color_distribution': {}\n",
        "        }\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                for line in block[\"lines\"]:\n",
        "                    for span in line.get(\"spans\", []):\n",
        "                        # Font size\n",
        "                        size = span.get(\"size\", 12)\n",
        "                        size_key = f\"{size:.1f}\"\n",
        "                        font_analysis['font_sizes'][size_key] = font_analysis['font_sizes'].get(size_key, 0) + 1\n",
        "\n",
        "                        # Font family\n",
        "                        font = span.get(\"font\", \"unknown\")\n",
        "                        font_analysis['font_families'][font] = font_analysis['font_families'].get(font, 0) + 1\n",
        "\n",
        "                        # Text style\n",
        "                        flags = span.get(\"flags\", 0)\n",
        "                        if flags & 2**4:  # Bold\n",
        "                            font_analysis['text_styles']['bold'] += 1\n",
        "                        elif flags & 2**1:  # Italic\n",
        "                            font_analysis['text_styles']['italic'] += 1\n",
        "                        else:\n",
        "                            font_analysis['text_styles']['normal'] += 1\n",
        "\n",
        "        return font_analysis\n",
        "\n",
        "    def _analyze_images_and_graphics(self, page) -> Dict:\n",
        "        \"\"\"Analyze images and graphics distribution\"\"\"\n",
        "        images = page.get_images()\n",
        "        drawings = page.get_drawings()\n",
        "\n",
        "        image_analysis = {\n",
        "            'image_count': len(images),\n",
        "            'drawing_count': len(drawings),\n",
        "            'image_positions': [],\n",
        "            'drawing_positions': []\n",
        "        }\n",
        "\n",
        "        # Analyze image positions\n",
        "        for img in images:\n",
        "            try:\n",
        "                bbox = page.get_image_bbox(img)\n",
        "                if bbox:\n",
        "                    image_analysis['image_positions'].append({\n",
        "                        'x': bbox.x0,\n",
        "                        'y': bbox.y0,\n",
        "                        'width': bbox.width,\n",
        "                        'height': bbox.height,\n",
        "                        'center_x': bbox.x0 + bbox.width / 2\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Analyze drawing positions\n",
        "        for drawing in drawings:\n",
        "            try:\n",
        "                rect = drawing.get(\"rect\")\n",
        "                if rect:\n",
        "                    image_analysis['drawing_positions'].append({\n",
        "                        'x': rect.x0,\n",
        "                        'y': rect.y0,\n",
        "                        'width': rect.width,\n",
        "                        'height': rect.height,\n",
        "                        'center_x': rect.x0 + rect.width / 2\n",
        "                    })\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return image_analysis\n",
        "\n",
        "    def _analyze_whitespace_distribution(self, page) -> Dict:\n",
        "        \"\"\"Analyze whitespace distribution\"\"\"\n",
        "        text_dict = page.get_text(\"dict\")\n",
        "        page_width = page.rect.width\n",
        "        page_height = page.rect.height\n",
        "\n",
        "        # Create occupancy map\n",
        "        occupied_areas = []\n",
        "\n",
        "        for block in text_dict.get(\"blocks\", []):\n",
        "            if \"lines\" in block:\n",
        "                bbox = block[\"bbox\"]\n",
        "                occupied_areas.append(bbox)\n",
        "\n",
        "        # Add images\n",
        "        images = page.get_images()\n",
        "        for img in images:\n",
        "            try:\n",
        "                bbox = page.get_image_bbox(img)\n",
        "                if bbox:\n",
        "                    occupied_areas.append([bbox.x0, bbox.y0, bbox.x1, bbox.y1])\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Find largest vertical gaps\n",
        "        vertical_gaps = []\n",
        "\n",
        "        if occupied_areas:\n",
        "            # Sort by x-coordinate\n",
        "            x_positions = []\n",
        "            for area in occupied_areas:\n",
        "                x_positions.extend([area[0], area[2]])\n",
        "\n",
        "            x_positions.sort()\n",
        "\n",
        "            for i in range(1, len(x_positions)):\n",
        "                gap = x_positions[i] - x_positions[i-1]\n",
        "                if gap > page_width * 0.02:  # Significant gap\n",
        "                    gap_center = (x_positions[i] + x_positions[i-1]) / 2\n",
        "                    vertical_gaps.append({\n",
        "                        'position': gap_center,\n",
        "                        'size': gap,\n",
        "                        'ratio': gap_center / page_width\n",
        "                    })\n",
        "\n",
        "        return {\n",
        "            'vertical_gaps': vertical_gaps,\n",
        "            'largest_gap': max(vertical_gaps, key=lambda g: g['size']) if vertical_gaps else None\n",
        "        }\n",
        "\n",
        "    def _combine_content_analyses(self, content_analysis: Dict) -> Dict:\n",
        "        \"\"\"Combine all content analyses\"\"\"\n",
        "\n",
        "        # Average the results across pages\n",
        "        combined = {\n",
        "            'recommended_split': 0.5,\n",
        "            'confidence': 0.1,\n",
        "            'method': 'content_analysis',\n",
        "            'evidence': []\n",
        "        }\n",
        "\n",
        "        split_suggestions = []\n",
        "        confidences = []\n",
        "\n",
        "        # From text density\n",
        "        for density in content_analysis['text_density_map']:\n",
        "            split_suggestions.append(density['optimal_split'])\n",
        "            confidences.append(0.3)  # Base confidence for density analysis\n",
        "\n",
        "        # From whitespace analysis\n",
        "        for whitespace in content_analysis['whitespace_analysis']:\n",
        "            if whitespace['largest_gap']:\n",
        "                gap = whitespace['largest_gap']\n",
        "                if 0.2 <= gap['ratio'] <= 0.8:  # Reasonable range\n",
        "                    split_suggestions.append(gap['ratio'])\n",
        "                    confidences.append(0.7)  # Higher confidence for clear gaps\n",
        "\n",
        "        # Calculate weighted average\n",
        "        if split_suggestions and confidences:\n",
        "            total_weight = sum(confidences)\n",
        "            weighted_split = sum(s * c for s, c in zip(split_suggestions, confidences)) / total_weight\n",
        "            avg_confidence = sum(confidences) / len(confidences)\n",
        "\n",
        "            combined['recommended_split'] = weighted_split\n",
        "            combined['confidence'] = avg_confidence\n",
        "\n",
        "        return combined\n",
        "\n",
        "    # =============================================================================\n",
        "    # üéØ MULTI-METHOD DETECTION SYSTEM\n",
        "    # =============================================================================\n",
        "\n",
        "    def detect_optimal_split_multi_method(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Use multiple methods to detect optimal split ratio\"\"\"\n",
        "\n",
        "        print(\"üéØ Running comprehensive multi-method analysis...\")\n",
        "\n",
        "        # Method 1: Line Detection\n",
        "        print(\"üìè Method 1: Vertical line detection...\")\n",
        "        line_analysis = self.detect_vertical_lines(pdf_path)\n",
        "\n",
        "        # Method 2: Content Analysis\n",
        "        print(\"üß† Method 2: Content layout analysis...\")\n",
        "        content_analysis = self.analyze_content_layout(pdf_path)\n",
        "\n",
        "        # Method 3: Visual Pattern Recognition\n",
        "        print(\"üëÅÔ∏è Method 3: Visual pattern recognition...\")\n",
        "        visual_analysis = self._analyze_visual_patterns(pdf_path)\n",
        "\n",
        "        # Method 4: Document Structure Analysis\n",
        "        print(\"üìã Method 4: Document structure analysis...\")\n",
        "        structure_analysis = self._analyze_document_structure(pdf_path)\n",
        "\n",
        "        # Combine all methods\n",
        "        print(\"üîÑ Combining all analysis methods...\")\n",
        "        final_analysis = self._combine_all_methods(\n",
        "            line_analysis, content_analysis, visual_analysis, structure_analysis\n",
        "        )\n",
        "\n",
        "        return final_analysis\n",
        "\n",
        "    def _analyze_visual_patterns(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Analyze visual patterns in the document\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            # Sample pages for visual analysis\n",
        "            sample_pages = min(3, len(doc))\n",
        "            visual_patterns = []\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                # Convert page to image for visual analysis\n",
        "                mat = fitz.Matrix(1.0, 1.0)  # No scaling\n",
        "                pix = page.get_pixmap(matrix=mat)\n",
        "\n",
        "                # Analyze pixel patterns (simplified)\n",
        "                page_analysis = {\n",
        "                    'page_width': page.rect.width,\n",
        "                    'page_height': page.rect.height,\n",
        "                    'visual_split_suggestions': []\n",
        "                }\n",
        "\n",
        "                # Look for clear vertical divisions in content\n",
        "                # This is a simplified approach - in practice, you might use image processing\n",
        "                text_blocks = page.get_text(\"dict\").get(\"blocks\", [])\n",
        "\n",
        "                if text_blocks:\n",
        "                    x_positions = []\n",
        "                    for block in text_blocks:\n",
        "                        if \"bbox\" in block:\n",
        "                            bbox = block[\"bbox\"]\n",
        "                            x_positions.extend([bbox[0], bbox[2]])\n",
        "\n",
        "                    if x_positions:\n",
        "                        x_positions.sort()\n",
        "\n",
        "                        # Find gaps\n",
        "                        for i in range(1, len(x_positions)):\n",
        "                            gap = x_positions[i] - x_positions[i-1]\n",
        "                            if gap > page.rect.width * 0.05:  # Significant gap\n",
        "                                gap_center = (x_positions[i] + x_positions[i-1]) / 2\n",
        "                                gap_ratio = gap_center / page.rect.width\n",
        "\n",
        "                                if 0.2 <= gap_ratio <= 0.8:\n",
        "                                    page_analysis['visual_split_suggestions'].append(gap_ratio)\n",
        "\n",
        "                visual_patterns.append(page_analysis)\n",
        "\n",
        "            # Combine visual analysis\n",
        "            all_suggestions = []\n",
        "            for pattern in visual_patterns:\n",
        "                all_suggestions.extend(pattern['visual_split_suggestions'])\n",
        "\n",
        "            if all_suggestions:\n",
        "                optimal_split = sum(all_suggestions) / len(all_suggestions)\n",
        "                confidence = min(1.0, len(all_suggestions) / 3)  # More suggestions = higher confidence\n",
        "            else:\n",
        "                optimal_split = 0.5\n",
        "                confidence = 0.1\n",
        "\n",
        "            return {\n",
        "                'optimal_split': optimal_split,\n",
        "                'confidence': confidence,\n",
        "                'method': 'visual_pattern_analysis',\n",
        "                'suggestions': all_suggestions\n",
        "            }\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _analyze_document_structure(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Analyze document structure and metadata\"\"\"\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        try:\n",
        "            structure_analysis = {\n",
        "                'page_count': len(doc),\n",
        "                'page_dimensions': [],\n",
        "                'text_statistics': {},\n",
        "                'structure_hints': []\n",
        "            }\n",
        "\n",
        "            # Analyze first few pages for structure\n",
        "            sample_pages = min(5, len(doc))\n",
        "            total_text = \"\"\n",
        "\n",
        "            for page_num in range(sample_pages):\n",
        "                page = doc[page_num]\n",
        "\n",
        "                structure_analysis['page_dimensions'].append({\n",
        "                    'width': page.rect.width,\n",
        "                    'height': page.rect.height,\n",
        "                    'ratio': page.rect.width / page.rect.height\n",
        "                })\n",
        "\n",
        "                page_text = page.get_text()\n",
        "                total_text += page_text\n",
        "\n",
        "            # Analyze text for structural hints\n",
        "            structure_hints = []\n",
        "\n",
        "            # Check for exam/question patterns\n",
        "            exam_keywords = ['question', 'answer', 'select', 'choose', 'option', 'practice', 'test', 'exam']\n",
        "            for keyword in exam_keywords:\n",
        "                if keyword.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"exam_paper_{keyword}\")\n",
        "\n",
        "            # Check for telegram/social media references\n",
        "            social_keywords = ['telegram', 'join', 'click here', 'open', 'subscribe']\n",
        "            for keyword in social_keywords:\n",
        "                if keyword.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"social_media_{keyword}\")\n",
        "\n",
        "            # Check for two-column indicators\n",
        "            column_indicators = ['column', 'left', 'right', 'side']\n",
        "            for indicator in column_indicators:\n",
        "                if indicator.lower() in total_text.lower():\n",
        "                    structure_hints.append(f\"column_layout_{indicator}\")\n",
        "\n",
        "            structure_analysis['structure_hints'] = structure_hints\n",
        "\n",
        "            # Determine likely split based on structure\n",
        "            if any('exam_paper' in hint for hint in structure_hints):\n",
        "                if any('social_media' in hint for hint in structure_hints):\n",
        "                    # Exam paper with social media reference - likely split needed\n",
        "                    suggested_split = 0.6  # Questions on left (60%), social media on right (40%)\n",
        "                    confidence = 0.8\n",
        "                else:\n",
        "                    # Pure exam paper - might not need split or different ratio\n",
        "                    suggested_split = 0.5\n",
        "                    confidence = 0.4\n",
        "            else:\n",
        "                # Unknown structure\n",
        "                suggested_split = 0.5\n",
        "                confidence = 0.2\n",
        "\n",
        "            return {\n",
        "                'optimal_split': suggested_split,\n",
        "                'confidence': confidence,\n",
        "                'method': 'document_structure_analysis',\n",
        "                'structure_hints': structure_hints,\n",
        "                'analysis': structure_analysis\n",
        "            }\n",
        "\n",
        "        finally:\n",
        "            doc.close()\n",
        "\n",
        "    def _combine_all_methods(self, line_analysis: Dict, content_analysis: Dict,\n",
        "                           visual_analysis: Dict, structure_analysis: Dict) -> Dict:\n",
        "        \"\"\"Combine results from all analysis methods\"\"\"\n",
        "\n",
        "        methods = [\n",
        "            ('line_detection', line_analysis),\n",
        "            ('content_analysis', content_analysis),\n",
        "            ('visual_analysis', visual_analysis),\n",
        "            ('structure_analysis', structure_analysis)\n",
        "        ]\n",
        "\n",
        "        # Weight methods by their confidence\n",
        "        weighted_splits = []\n",
        "        weighted_confidences = []\n",
        "        method_details = {}\n",
        "\n",
        "        for method_name, analysis in methods:\n",
        "            split = analysis.get('optimal_split', 0.5)\n",
        "            confidence = analysis.get('confidence', 0.1)\n",
        "\n",
        "            weighted_splits.append(split * confidence)\n",
        "            weighted_confidences.append(confidence)\n",
        "\n",
        "            method_details[method_name] = {\n",
        "                'split': split,\n",
        "                'confidence': confidence,\n",
        "                'details': analysis\n",
        "            }\n",
        "\n",
        "        # Calculate final weighted average\n",
        "        total_weight = sum(weighted_confidences)\n",
        "\n",
        "        if total_weight > 0:\n",
        "            final_split = sum(weighted_splits) / total_weight\n",
        "            final_confidence = sum(weighted_confidences) / len(weighted_confidences)\n",
        "        else:\n",
        "            final_split = 0.5\n",
        "            final_confidence = 0.1\n",
        "\n",
        "        # Adjust confidence based on method agreement\n",
        "        splits_only = [analysis.get('optimal_split', 0.5) for _, analysis in methods]\n",
        "        split_variance = np.var(splits_only) if len(splits_only) > 1 else 0\n",
        "        agreement_factor = max(0.1, 1 - split_variance * 5)  # Higher variance = lower agreement\n",
        "\n",
        "        final_confidence *= agreement_factor\n",
        "\n",
        "        return {\n",
        "            'optimal_split': final_split,\n",
        "            'confidence': final_confidence,\n",
        "            'agreement_factor': agreement_factor,\n",
        "            'method': 'multi_method_combined',\n",
        "            'individual_methods': method_details,\n",
        "            'summary': {\n",
        "                'line_detection_confidence': line_analysis.get('confidence', 0),\n",
        "                'content_analysis_confidence': content_analysis.get('confidence', 0),\n",
        "                'visual_analysis_confidence': visual_analysis.get('confidence', 0),\n",
        "                'structure_analysis_confidence': structure_analysis.get('confidence', 0),\n",
        "                'methods_agreement': agreement_factor,\n",
        "                'recommended_split': final_split\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # =============================================================================\n",
        "    # ‚úÇÔ∏è PDF SPLITTING ENGINE\n",
        "    # =============================================================================\n",
        "\n",
        "    def split_pdf_with_analysis(self, input_path: str, output_path: str,\n",
        "                               method: str = \"multi_method\") -> bool:\n",
        "        \"\"\"Split PDF using comprehensive analysis\"\"\"\n",
        "\n",
        "        input_doc = None\n",
        "        output_doc = None\n",
        "\n",
        "        try:\n",
        "            # Step 1: Comprehensive Analysis\n",
        "            print(\"üîç Step 1: Comprehensive PDF Analysis\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            if method == \"multi_method\":\n",
        "                analysis = self.detect_optimal_split_multi_method(input_path)\n",
        "            elif method == \"line_detection\":\n",
        "                analysis = self.detect_vertical_lines(input_path)\n",
        "            elif method == \"content_analysis\":\n",
        "                analysis = self.analyze_content_layout(input_path)\n",
        "            else:\n",
        "                # Fallback to multi-method\n",
        "                analysis = self.detect_optimal_split_multi_method(input_path)\n",
        "\n",
        "            optimal_ratio = analysis.get('optimal_split', 0.5)\n",
        "            confidence = analysis.get('confidence', 0.1)\n",
        "\n",
        "            print(f\"\\nüìä Analysis Results:\")\n",
        "            print(f\"   ‚Ä¢ Detection method: {analysis.get('method', 'unknown')}\")\n",
        "            print(f\"   ‚Ä¢ Optimal split ratio: {optimal_ratio:.1%}\")\n",
        "            print(f\"   ‚Ä¢ Detection confidence: {confidence:.1%}\")\n",
        "\n",
        "            if 'summary' in analysis:\n",
        "                summary = analysis['summary']\n",
        "                print(f\"   ‚Ä¢ Line detection confidence: {summary.get('line_detection_confidence', 0):.1%}\")\n",
        "                print(f\"   ‚Ä¢ Content analysis confidence: {summary.get('content_analysis_confidence', 0):.1%}\")\n",
        "                print(f\"   ‚Ä¢ Methods agreement: {summary.get('methods_agreement', 0):.1%}\")\n",
        "\n",
        "            # Validate the detected ratio\n",
        "            if optimal_ratio < 0.15 or optimal_ratio > 0.85:\n",
        "                print(f\"‚ö†Ô∏è Warning: Detected ratio {optimal_ratio:.1%} seems extreme. Using safer ratio.\")\n",
        "                optimal_ratio = max(0.2, min(0.8, optimal_ratio))\n",
        "\n",
        "            if confidence < 0.3:\n",
        "                print(f\"‚ö†Ô∏è Warning: Low confidence ({confidence:.1%}). Results may not be optimal.\")\n",
        "\n",
        "            # Step 2: Apply Split\n",
        "            print(f\"\\n‚úÇÔ∏è Step 2: Applying Split at {optimal_ratio:.1%}\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            input_doc = fitz.open(input_path)\n",
        "            output_doc = fitz.open()\n",
        "\n",
        "            page_count = len(input_doc)\n",
        "            success_count = 0\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            with tqdm(total=page_count, desc=\"üìÑ Processing\", unit=\"page\") as pbar:\n",
        "                for page_num in range(page_count):\n",
        "                    try:\n",
        "                        page = input_doc[page_num]\n",
        "                        rect = page.rect\n",
        "\n",
        "                        # Calculate split position\n",
        "                        split_pos = rect.width * optimal_ratio\n",
        "\n",
        "                        # Create left half\n",
        "                        left_clip = fitz.Rect(0, 0, split_pos, rect.height)\n",
        "                        left_page = output_doc.new_page(width=split_pos, height=rect.height)\n",
        "                        left_page.show_pdf_page(fitz.Rect(0, 0, split_pos, rect.height),\n",
        "                                               input_doc, page_num, clip=left_clip)\n",
        "\n",
        "                        # Create right half\n",
        "                        right_clip = fitz.Rect(split_pos, 0, rect.width, rect.height)\n",
        "                        right_page = output_doc.new_page(width=rect.width - split_pos, height=rect.height)\n",
        "                        right_page.show_pdf_page(fitz.Rect(0, 0, rect.width - split_pos, rect.height),\n",
        "                                                input_doc, page_num, clip=right_clip)\n",
        "\n",
        "                        success_count += 1\n",
        "                        pbar.set_postfix({\n",
        "                            'Split': f\"{optimal_ratio:.1%}\",\n",
        "                            'Success': f\"{success_count}/{page_count}\"\n",
        "                        })\n",
        "                        pbar.update(1)\n",
        "\n",
        "                        # Memory management\n",
        "                        if (page_num + 1) % 10 == 0:\n",
        "                            gc.collect()\n",
        "\n",
        "                    except Exception as page_error:\n",
        "                        print(f\"‚ö†Ô∏è Page {page_num + 1} error: {page_error}\")\n",
        "                        continue\n",
        "\n",
        "            # Step 3: Save Results\n",
        "            print(\"\\nüíæ Step 3: Saving Results\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            if len(output_doc) == 0:\n",
        "                print(\"‚ùå No pages were processed successfully\")\n",
        "                return False\n",
        "\n",
        "            try:\n",
        "                # Save with optimization\n",
        "                output_doc.save(output_path, garbage=4, deflate=True)\n",
        "\n",
        "                # Calculate performance metrics\n",
        "                elapsed_time = time.time() - start_time\n",
        "                output_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "                created_pages = len(output_doc)\n",
        "\n",
        "                print(f\"‚úÖ Processing Complete!\")\n",
        "                print(f\"\\nüìä Results Summary:\")\n",
        "                print(f\"   ‚Ä¢ Input file: {os.path.basename(input_path)}\")\n",
        "                print(f\"   ‚Ä¢ Output file: {os.path.basename(output_path)}\")\n",
        "                print(f\"   ‚Ä¢ Pages processed: {success_count}/{page_count}\")\n",
        "                print(f\"   ‚Ä¢ Pages created: {created_pages}\")\n",
        "                print(f\"   ‚Ä¢ Output size: {output_size:.2f} MB\")\n",
        "                print(f\"   ‚Ä¢ Processing time: {elapsed_time:.2f} seconds\")\n",
        "                print(f\"   ‚Ä¢ Speed: {page_count/elapsed_time:.1f} pages/second\")\n",
        "                print(f\"   ‚Ä¢ Split ratio used: {optimal_ratio:.1%}\")\n",
        "                print(f\"   ‚Ä¢ Detection confidence: {confidence:.1%}\")\n",
        "                print(f\"   ‚Ä¢ Success rate: {success_count/page_count:.1%}\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            except Exception as save_error:\n",
        "                print(f\"‚ùå Save error: {save_error}\")\n",
        "                # Try basic save as fallback\n",
        "                try:\n",
        "                    output_doc.save(output_path)\n",
        "                    print(\"‚úÖ Saved with basic options\")\n",
        "                    return True\n",
        "                except Exception as basic_error:\n",
        "                    print(f\"‚ùå Basic save failed: {basic_error}\")\n",
        "                    return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during processing: {e}\")\n",
        "            import traceback\n",
        "            if self.debug_mode:\n",
        "                print(f\"üîç Debug traceback:\\n{traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "        finally:\n",
        "            # Cleanup\n",
        "            if input_doc:\n",
        "                try:\n",
        "                    input_doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            if output_doc:\n",
        "                try:\n",
        "                    output_doc.close()\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            gc.collect()\n",
        "\n",
        "    # =============================================================================\n",
        "    # üéõÔ∏è USER INTERFACE AND CONTROL FUNCTIONS\n",
        "    # =============================================================================\n",
        "\n",
        "    def create_advanced_interface(self):\n",
        "        \"\"\"Create advanced user interface\"\"\"\n",
        "\n",
        "        print(\"üéõÔ∏è ADVANCED PDF SPLITTER INTERFACE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Detection method selection\n",
        "        method_selector = widgets.Dropdown(\n",
        "            options=[\n",
        "                ('üéØ Multi-Method Analysis (Recommended)', 'multi_method'),\n",
        "                ('üìè Line Detection Only', 'line_detection'),\n",
        "                ('üß† Content Analysis Only', 'content_analysis'),\n",
        "                ('üëÅÔ∏è Visual Analysis Only', 'visual_analysis'),\n",
        "                ('üîß Custom Ratio', 'custom')\n",
        "            ],\n",
        "            value='multi_method',\n",
        "            description='Analysis Method:',\n",
        "            style={'description_width': '150px'},\n",
        "            layout={'width': '400px'}\n",
        "        )\n",
        "\n",
        "        # Custom ratio slider (hidden by default)\n",
        "        custom_ratio_slider = widgets.FloatSlider(\n",
        "            value=0.5,\n",
        "            min=0.1,\n",
        "            max=0.9,\n",
        "            step=0.01,\n",
        "            description='Custom Split Ratio:',\n",
        "            style={'description_width': '150px'},\n",
        "            readout_format='.0%',\n",
        "            layout={'width': '400px', 'display': 'none'}\n",
        "        )\n",
        "\n",
        "        # Advanced options\n",
        "        debug_checkbox = widgets.Checkbox(\n",
        "            value=True,\n",
        "            description='Enable detailed analysis output',\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        sample_pages_slider = widgets.IntSlider(\n",
        "            value=10,\n",
        "            min=1,\n",
        "            max=20,\n",
        "            description='Pages to analyze:',\n",
        "            style={'description_width': '150px'},\n",
        "            layout={'width': '400px'}\n",
        "        )\n",
        "\n",
        "        # Analysis results display\n",
        "        results_output = widgets.Output()\n",
        "\n",
        "        # Interactive elements\n",
        "        def on_method_change(change):\n",
        "            if change['new'] == 'custom':\n",
        "                custom_ratio_slider.layout.display = 'block'\n",
        "            else:\n",
        "                custom_ratio_slider.layout.display = 'none'\n",
        "\n",
        "        method_selector.observe(on_method_change, names='value')\n",
        "\n",
        "        # Layout\n",
        "        interface = widgets.VBox([\n",
        "            widgets.HTML(\"<h2>üéØ Advanced PDF Splitter Configuration</h2>\"),\n",
        "            widgets.HTML(\"<p>Select analysis method and configure options:</p>\"),\n",
        "            method_selector,\n",
        "            custom_ratio_slider,\n",
        "            debug_checkbox,\n",
        "            sample_pages_slider,\n",
        "            widgets.HTML(\"<hr>\"),\n",
        "            results_output\n",
        "        ])\n",
        "\n",
        "        display(interface)\n",
        "\n",
        "        return {\n",
        "            'method_selector': method_selector,\n",
        "            'custom_ratio_slider': custom_ratio_slider,\n",
        "            'debug_checkbox': debug_checkbox,\n",
        "            'sample_pages_slider': sample_pages_slider,\n",
        "            'results_output': results_output\n",
        "        }\n",
        "\n",
        "    def validate_pdf_file(self, pdf_path: str) -> Tuple[bool, str, Dict]:\n",
        "        \"\"\"Comprehensive PDF validation\"\"\"\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(pdf_path):\n",
        "                return False, \"‚ùå File not found\", {}\n",
        "\n",
        "            if not pdf_path.lower().endswith('.pdf'):\n",
        "                return False, \"‚ùå Not a PDF file\", {}\n",
        "\n",
        "            # Get file info\n",
        "            file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB\n",
        "            file_modified = datetime.fromtimestamp(os.path.getmtime(pdf_path))\n",
        "\n",
        "            # Open and analyze PDF\n",
        "            doc = fitz.open(pdf_path)\n",
        "\n",
        "            try:\n",
        "                page_count = len(doc)\n",
        "\n",
        "                if page_count == 0:\n",
        "                    return False, \"‚ùå PDF has no pages\", {}\n",
        "\n",
        "                if doc.needs_pass:\n",
        "                    return False, \"‚ùå PDF is password protected\", {}\n",
        "\n",
        "                # Get detailed info\n",
        "                metadata = doc.metadata\n",
        "                first_page = doc[0]\n",
        "\n",
        "                pdf_info = {\n",
        "                    'file_size_mb': file_size,\n",
        "                    'page_count': page_count,\n",
        "                    'page_dimensions': {\n",
        "                        'width': first_page.rect.width,\n",
        "                        'height': first_page.rect.height,\n",
        "                        'aspect_ratio': first_page.rect.width / first_page.rect.height\n",
        "                    },\n",
        "                    'metadata': {\n",
        "                        'title': metadata.get('title', 'Unknown'),\n",
        "                        'author': metadata.get('author', 'Unknown'),\n",
        "                        'creator': metadata.get('creator', 'Unknown'),\n",
        "                        'producer': metadata.get('producer', 'Unknown')\n",
        "                    },\n",
        "                    'file_modified': file_modified.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'estimated_processing_time': page_count * 0.1  # seconds\n",
        "                }\n",
        "\n",
        "                # Check for potential issues\n",
        "                warnings = []\n",
        "\n",
        "                if file_size > 50:  # Large file\n",
        "                    warnings.append(f\"‚ö†Ô∏è Large file ({file_size:.1f} MB) - processing may take longer\")\n",
        "\n",
        "                if page_count > 100:\n",
        "                    warnings.append(f\"‚ö†Ô∏è Many pages ({page_count}) - consider processing in batches\")\n",
        "\n",
        "                if first_page.rect.width / first_page.rect.height < 1.2:\n",
        "                    warnings.append(\"‚ö†Ô∏è Pages seem narrow - vertical split may not be optimal\")\n",
        "\n",
        "                success_message = f\"‚úÖ Valid PDF: {page_count} pages, {file_size:.1f} MB\"\n",
        "                if warnings:\n",
        "                    success_message += f\"\\n   {'   '.join(warnings)}\"\n",
        "\n",
        "                return True, success_message, pdf_info\n",
        "\n",
        "            finally:\n",
        "                doc.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"‚ùå Error analyzing PDF: {str(e)}\", {}\n",
        "\n",
        "    def download_with_retry(self, file_path: str, max_retries: int = 3) -> bool:\n",
        "        \"\"\"Download file with retry logic and better error handling\"\"\"\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"‚ùå File not found: {file_path}\")\n",
        "            return False\n",
        "\n",
        "        file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"üì• Preparing download: {os.path.basename(file_path)} ({file_size:.2f} MB)\")\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"   Attempt {attempt + 1}/{max_retries}...\")\n",
        "                files.download(file_path)\n",
        "                print(\"‚úÖ Download successful!\")\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
        "\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (attempt + 1) * 2  # Progressive wait\n",
        "                    print(f\"   ‚è≥ Waiting {wait_time} seconds before retry...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"‚ùå All download attempts failed\")\n",
        "                    print(f\"üí° File is still available locally: {file_path}\")\n",
        "                    return False\n",
        "\n",
        "        return False\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ MAIN EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def test_comprehensive_analysis():\n",
        "    \"\"\"Test comprehensive analysis on uploaded PDF\"\"\"\n",
        "\n",
        "    print(\"üß™ COMPREHENSIVE PDF ANALYSIS TEST\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üïê Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Find PDF files\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found.\")\n",
        "        print(\"üí° Please upload a PDF file first.\")\n",
        "        return None\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"üìÅ Analyzing: {pdf_file}\")\n",
        "\n",
        "    # Create splitter instance\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Validate PDF\n",
        "    is_valid, message, pdf_info = splitter.validate_pdf_file(pdf_file)\n",
        "    print(f\"\\nüìã PDF Validation:\")\n",
        "    print(message)\n",
        "\n",
        "    if not is_valid:\n",
        "        return None\n",
        "\n",
        "    if pdf_info:\n",
        "        print(f\"\\nüìä PDF Information:\")\n",
        "        print(f\"   ‚Ä¢ Pages: {pdf_info['page_count']}\")\n",
        "        print(f\"   ‚Ä¢ Size: {pdf_info['file_size_mb']:.2f} MB\")\n",
        "        print(f\"   ‚Ä¢ Dimensions: {pdf_info['page_dimensions']['width']:.0f} x {pdf_info['page_dimensions']['height']:.0f}\")\n",
        "        print(f\"   ‚Ä¢ Aspect ratio: {pdf_info['page_dimensions']['aspect_ratio']:.2f}\")\n",
        "        print(f\"   ‚Ä¢ Estimated processing time: {pdf_info['estimated_processing_time']:.1f} seconds\")\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    print(f\"\\nüîç Running Comprehensive Analysis...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    analysis_result = splitter.detect_optimal_split_multi_method(pdf_file)\n",
        "\n",
        "    # Display detailed results\n",
        "    print(f\"\\nüìä COMPREHENSIVE ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üéØ Optimal Split Ratio: {analysis_result['optimal_split']:.1%}\")\n",
        "    print(f\"üîí Overall Confidence: {analysis_result['confidence']:.1%}\")\n",
        "    print(f\"ü§ù Methods Agreement: {analysis_result.get('agreement_factor', 0):.1%}\")\n",
        "\n",
        "    if 'summary' in analysis_result:\n",
        "        summary = analysis_result['summary']\n",
        "        print(f\"\\nüìã Individual Method Confidences:\")\n",
        "        print(f\"   üìè Line Detection: {summary.get('line_detection_confidence', 0):.1%}\")\n",
        "        print(f\"   üß† Content Analysis: {summary.get('content_analysis_confidence', 0):.1%}\")\n",
        "        print(f\"   üëÅÔ∏è Visual Analysis: {summary.get('visual_analysis_confidence', 0):.1%}\")\n",
        "        print(f\"   üìã Structure Analysis: {summary.get('structure_analysis_confidence', 0):.1%}\")\n",
        "\n",
        "    # Recommendations\n",
        "    confidence = analysis_result['confidence']\n",
        "    split_ratio = analysis_result['optimal_split']\n",
        "\n",
        "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    if confidence > 0.8:\n",
        "        print(\"üü¢ EXCELLENT: Very high confidence in detection\")\n",
        "        print(f\"   ‚úÖ Proceed with split at {split_ratio:.1%}\")\n",
        "    elif confidence > 0.6:\n",
        "        print(\"üü° GOOD: High confidence in detection\")\n",
        "        print(f\"   ‚úÖ Recommended split at {split_ratio:.1%}\")\n",
        "    elif confidence > 0.4:\n",
        "        print(\"üü† MODERATE: Moderate confidence\")\n",
        "        print(f\"   ‚ö†Ô∏è Consider manual review of {split_ratio:.1%} split\")\n",
        "    else:\n",
        "        print(\"üî¥ LOW: Low confidence in detection\")\n",
        "        print(f\"   ‚ö†Ô∏è Manual inspection recommended\")\n",
        "        print(f\"   üí≠ Suggested fallback: {split_ratio:.1%}\")\n",
        "\n",
        "    # Special cases\n",
        "    if split_ratio < 0.25 or split_ratio > 0.75:\n",
        "        print(f\"   ‚ö†Ô∏è Unusual split ratio detected: {split_ratio:.1%}\")\n",
        "        print(\"   üí≠ This might indicate special document layout\")\n",
        "\n",
        "    return analysis_result\n",
        "\n",
        "def run_intelligent_split():\n",
        "    \"\"\"Run intelligent PDF splitting with comprehensive analysis\"\"\"\n",
        "\n",
        "    print(\"üéØ INTELLIGENT PDF SPLITTER v4.0\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üß† Complete Multi-Method Analysis System\")\n",
        "    print(f\"üïê Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Find PDF files\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found.\")\n",
        "        print(\"üí° Please upload a PDF file first.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "\n",
        "    # Create splitter\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Generate output filename\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    output_file = f\"INTELLIGENT_SPLIT_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "    print(f\"üìÅ Input: {pdf_file}\")\n",
        "    print(f\"üìÅ Output: {output_file}\")\n",
        "\n",
        "    # Process with comprehensive analysis\n",
        "    success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "    if success:\n",
        "        print(f\"\\nüéâ SUCCESS! PDF split completed.\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Attempt download\n",
        "        if splitter.download_with_retry(output_file):\n",
        "            print(\"‚úÖ File downloaded successfully!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Download failed, but file is ready locally\")\n",
        "def run_interactive_mode():\n",
        "    \"\"\"Run interactive mode with step-by-step guidance\"\"\"\n",
        "\n",
        "    print(\"üéÆ INTERACTIVE PDF SPLITTER MODE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üïê Current Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ Current User: Ravi-katta-dev\")\n",
        "    print(\"üéØ Interactive Step-by-Step Processing\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create splitter instance\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Step 1: Find and validate PDF\n",
        "    print(\"\\nüìã STEP 1: PDF Discovery and Validation\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found in current directory.\")\n",
        "        print(\"\\nüí° To upload a PDF:\")\n",
        "        print(\"   1. Click the üìÅ folder icon in the left sidebar\")\n",
        "        print(\"   2. Select 'Upload to session storage'\")\n",
        "        print(\"   3. Choose your PDF file\")\n",
        "        print(\"   4. Wait for upload to complete\")\n",
        "        print(\"   5. Re-run this function\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Found PDF files:\")\n",
        "    for i, pdf in enumerate(pdf_files, 1):\n",
        "        file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "        print(f\"   {i}. {pdf} ({file_size:.2f} MB)\")\n",
        "\n",
        "    # Use first PDF\n",
        "    selected_pdf = pdf_files[0]\n",
        "    print(f\"\\nüéØ Selected: {selected_pdf}\")\n",
        "\n",
        "    # Validate PDF\n",
        "    is_valid, validation_message, pdf_info = splitter.validate_pdf_file(selected_pdf)\n",
        "    print(f\"\\nüìä Validation Result:\")\n",
        "    print(validation_message)\n",
        "\n",
        "    if not is_valid:\n",
        "        print(\"‚ùå Cannot proceed with invalid PDF.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Quick Analysis Preview\n",
        "    print(f\"\\nüîç STEP 2: Quick Analysis Preview\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"Running quick line detection preview...\")\n",
        "    quick_analysis = splitter.detect_vertical_lines(selected_pdf, sample_pages=3)\n",
        "\n",
        "    quick_split = quick_analysis.get('optimal_split', 0.5)\n",
        "    quick_confidence = quick_analysis.get('confidence', 0.1)\n",
        "\n",
        "    print(f\"üìè Quick line detection result:\")\n",
        "    print(f\"   ‚Ä¢ Suggested split: {quick_split:.1%}\")\n",
        "    print(f\"   ‚Ä¢ Confidence: {quick_confidence:.1%}\")\n",
        "\n",
        "    if quick_confidence > 0.6:\n",
        "        print(\"‚úÖ High confidence - line detection working well!\")\n",
        "    elif quick_confidence > 0.3:\n",
        "        print(\"‚ö†Ô∏è Moderate confidence - will use comprehensive analysis\")\n",
        "    else:\n",
        "        print(\"‚ùå Low confidence - will run full multi-method analysis\")\n",
        "\n",
        "    # Step 3: User Decision Point\n",
        "    print(f\"\\nü§î STEP 3: Processing Decision\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if quick_confidence > 0.7:\n",
        "        print(\"üöÄ RECOMMENDATION: Use quick line detection\")\n",
        "        print(f\"   Split ratio: {quick_split:.1%}\")\n",
        "        print(\"   This should work perfectly for your PDF!\")\n",
        "        recommended_method = \"line_detection\"\n",
        "    else:\n",
        "        print(\"üß† RECOMMENDATION: Use comprehensive multi-method analysis\")\n",
        "        print(\"   This will analyze your PDF using all available methods\")\n",
        "        recommended_method = \"multi_method\"\n",
        "\n",
        "    print(f\"\\nüìù Available options:\")\n",
        "    print(\"   1. üöÄ Use recommended method (fastest)\")\n",
        "    print(\"   2. üß† Force comprehensive analysis (most accurate)\")\n",
        "    print(\"   3. üìè Line detection only\")\n",
        "    print(\"   4. üéõÔ∏è Custom ratio\")\n",
        "    print(\"   5. üß™ Test all methods (analysis only)\")\n",
        "\n",
        "    # Auto-select recommended method for demo\n",
        "    choice = \"1\"  # Simulating user choosing recommended method\n",
        "    print(f\"üéØ Auto-selecting option 1 (recommended method)\")\n",
        "\n",
        "    # Step 4: Execute Processing\n",
        "    print(f\"\\n‚öôÔ∏è STEP 4: Processing Execution\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Generate output filename\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        method = recommended_method\n",
        "        output_file = f\"SMART_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"2\":\n",
        "        method = \"multi_method\"\n",
        "        output_file = f\"COMPREHENSIVE_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"3\":\n",
        "        method = \"line_detection\"\n",
        "        output_file = f\"LINE_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    elif choice == \"4\":\n",
        "        # Custom ratio would be handled here\n",
        "        custom_ratio = 0.6  # Example\n",
        "        method = \"custom\"\n",
        "        output_file = f\"CUSTOM_SPLIT_{Path(selected_pdf).stem}_{timestamp}.pdf\"\n",
        "    else:\n",
        "        # Test mode\n",
        "        print(\"üß™ Running comprehensive test...\")\n",
        "        test_result = test_comprehensive_analysis()\n",
        "        print(\"‚úÖ Test completed! Check results above.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìÅ Output file: {output_file}\")\n",
        "    print(f\"üîß Processing method: {method}\")\n",
        "\n",
        "    # Execute the split\n",
        "    print(f\"\\nüöÄ Starting processing...\")\n",
        "    success = splitter.split_pdf_with_analysis(selected_pdf, output_file, method)\n",
        "\n",
        "    # Step 5: Results and Download\n",
        "    print(f\"\\nüì¶ STEP 5: Results and Download\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if success:\n",
        "        print(\"üéâ SUCCESS! PDF processing completed.\")\n",
        "\n",
        "        # File statistics\n",
        "        if os.path.exists(output_file):\n",
        "            output_size = os.path.getsize(output_file) / (1024 * 1024)\n",
        "            input_size = os.path.getsize(selected_pdf) / (1024 * 1024)\n",
        "\n",
        "            print(f\"\\nüìä File Statistics:\")\n",
        "            print(f\"   ‚Ä¢ Input size: {input_size:.2f} MB\")\n",
        "            print(f\"   ‚Ä¢ Output size: {output_size:.2f} MB\")\n",
        "            print(f\"   ‚Ä¢ Size efficiency: {(1 - output_size/input_size):.1%} reduction\")\n",
        "\n",
        "        # Attempt download\n",
        "        print(f\"\\nüì• Attempting download...\")\n",
        "        download_success = splitter.download_with_retry(output_file, max_retries=3)\n",
        "\n",
        "        if download_success:\n",
        "            print(\"‚úÖ Download completed successfully!\")\n",
        "            print(f\"üíæ Your split PDF has been downloaded: {output_file}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Download failed, but processing was successful\")\n",
        "            print(f\"üí° File is available locally: {output_file}\")\n",
        "            print(\"üîÑ You can try downloading manually or re-run the download\")\n",
        "    else:\n",
        "        print(\"‚ùå Processing failed. Please check the error messages above.\")\n",
        "\n",
        "        # Troubleshooting suggestions\n",
        "        print(f\"\\nüîß Troubleshooting suggestions:\")\n",
        "        print(\"   1. Check if PDF is valid and not corrupted\")\n",
        "        print(\"   2. Try restarting the runtime (Runtime ‚Üí Restart Runtime)\")\n",
        "        print(\"   3. Re-upload the PDF file\")\n",
        "        print(\"   4. Try a different processing method\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Interactive session completed!\")\n",
        "    print(f\"üïê Session ended: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
        "\n",
        "def quick_split_now():\n",
        "    \"\"\"Quick split with minimal setup - just process and download\"\"\"\n",
        "\n",
        "    print(\"‚ö° QUICK SPLIT - FAST PROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üïê Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(\"‚ö° Fast processing with smart defaults\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find PDF\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found.\")\n",
        "        return\n",
        "\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"üìÅ Processing: {pdf_file}\")\n",
        "\n",
        "    # Quick validation\n",
        "    if not os.path.exists(pdf_file):\n",
        "        print(\"‚ùå File not found.\")\n",
        "        return\n",
        "\n",
        "    file_size = os.path.getsize(pdf_file) / (1024 * 1024)\n",
        "    print(f\"üìä File size: {file_size:.2f} MB\")\n",
        "\n",
        "    # Create splitter and process\n",
        "    splitter = CompletePDFSplitter()\n",
        "    splitter.debug_mode = False  # Reduce output for quick mode\n",
        "\n",
        "    # Generate output\n",
        "    timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "    output_file = f\"QUICK_SPLIT_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "    print(f\"üöÄ Quick processing...\")\n",
        "\n",
        "    # Use multi-method for best results\n",
        "    success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "    if success:\n",
        "        print(f\"‚úÖ Done! Downloading {output_file}...\")\n",
        "\n",
        "        try:\n",
        "            files.download(output_file)\n",
        "            print(\"üéâ Success! Your split PDF has been downloaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Download error: {e}\")\n",
        "            print(f\"üí° File saved as: {output_file}\")\n",
        "    else:\n",
        "        print(\"‚ùå Processing failed.\")\n",
        "\n",
        "def batch_process_pdfs():\n",
        "    \"\"\"Process multiple PDFs if available\"\"\"\n",
        "\n",
        "    print(\"üì¶ BATCH PDF PROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üïê Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(\"üì¶ Process multiple PDFs automatically\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find all PDFs\n",
        "    all_pdfs = [f for f in os.listdir('.') if f.lower().endswith('.pdf')]\n",
        "    input_pdfs = [f for f in all_pdfs if not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_', 'QUICK_'))]\n",
        "\n",
        "    if not input_pdfs:\n",
        "        print(\"‚ùå No input PDF files found.\")\n",
        "        return\n",
        "\n",
        "    if len(input_pdfs) == 1:\n",
        "        print(f\"üìÑ Only one PDF found: {input_pdfs[0]}\")\n",
        "        print(\"üí° Use quick_split_now() for single file processing\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìö Found {len(input_pdfs)} PDFs to process:\")\n",
        "    for i, pdf in enumerate(input_pdfs, 1):\n",
        "        file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "        print(f\"   {i}. {pdf} ({file_size:.2f} MB)\")\n",
        "\n",
        "    # Create splitter\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    # Process each PDF\n",
        "    successful_files = []\n",
        "    failed_files = []\n",
        "\n",
        "    for i, pdf_file in enumerate(input_pdfs, 1):\n",
        "        print(f\"\\nüîÑ Processing {i}/{len(input_pdfs)}: {pdf_file}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Generate output name\n",
        "        timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "        output_file = f\"BATCH_SPLIT_{i}_{Path(pdf_file).stem}_{timestamp}.pdf\"\n",
        "\n",
        "        try:\n",
        "            success = splitter.split_pdf_with_analysis(pdf_file, output_file, \"multi_method\")\n",
        "\n",
        "            if success:\n",
        "                successful_files.append(output_file)\n",
        "                print(f\"‚úÖ {pdf_file} processed successfully\")\n",
        "            else:\n",
        "                failed_files.append(pdf_file)\n",
        "                print(f\"‚ùå {pdf_file} processing failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_files.append(pdf_file)\n",
        "            print(f\"‚ùå {pdf_file} error: {e}\")\n",
        "\n",
        "    # Results summary\n",
        "    print(f\"\\nüìä BATCH PROCESSING SUMMARY\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"‚úÖ Successful: {len(successful_files)}/{len(input_pdfs)}\")\n",
        "    print(f\"‚ùå Failed: {len(failed_files)}/{len(input_pdfs)}\")\n",
        "\n",
        "    if successful_files:\n",
        "        print(f\"\\nüì• Downloading successful files...\")\n",
        "\n",
        "        for output_file in successful_files:\n",
        "            try:\n",
        "                files.download(output_file)\n",
        "                print(f\"‚úÖ Downloaded: {output_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Download failed: {output_file} - {e}\")\n",
        "\n",
        "    if failed_files:\n",
        "        print(f\"\\n‚ùå Failed files:\")\n",
        "        for failed_file in failed_files:\n",
        "            print(f\"   ‚Ä¢ {failed_file}\")\n",
        "\n",
        "def diagnose_pdf_issues():\n",
        "    \"\"\"Diagnose potential issues with PDF processing\"\"\"\n",
        "\n",
        "    print(\"üîç PDF DIAGNOSTIC TOOL\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üïê Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(\"üîç Comprehensive PDF analysis and issue detection\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Find PDFs\n",
        "    pdf_files = [f for f in os.listdir('.') if f.lower().endswith('.pdf') and not f.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_'))]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDF files found for diagnosis.\")\n",
        "        return\n",
        "\n",
        "    splitter = CompletePDFSplitter()\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nüîç DIAGNOSING: {pdf_file}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Basic file info\n",
        "        try:\n",
        "            file_size = os.path.getsize(pdf_file) / (1024 * 1024)\n",
        "            print(f\"üìÅ File size: {file_size:.2f} MB\")\n",
        "\n",
        "            # Detailed validation\n",
        "            is_valid, message, pdf_info = splitter.validate_pdf_file(pdf_file)\n",
        "            print(f\"üìã Validation: {message}\")\n",
        "\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Open PDF for detailed analysis\n",
        "            doc = fitz.open(pdf_file)\n",
        "\n",
        "            try:\n",
        "                # Page analysis\n",
        "                print(f\"\\nüìÑ Page Analysis:\")\n",
        "                print(f\"   ‚Ä¢ Total pages: {len(doc)}\")\n",
        "\n",
        "                if len(doc) > 0:\n",
        "                    first_page = doc[0]\n",
        "                    print(f\"   ‚Ä¢ Page size: {first_page.rect.width:.0f} x {first_page.rect.height:.0f}\")\n",
        "                    print(f\"   ‚Ä¢ Aspect ratio: {first_page.rect.width/first_page.rect.height:.2f}\")\n",
        "\n",
        "                    # Text analysis\n",
        "                    text_content = first_page.get_text()\n",
        "                    print(f\"   ‚Ä¢ Text length (first page): {len(text_content)} characters\")\n",
        "\n",
        "                    if len(text_content) < 50:\n",
        "                        print(\"   ‚ö†Ô∏è Warning: Very little text detected\")\n",
        "\n",
        "                    # Check for common keywords\n",
        "                    keywords = ['question', 'answer', 'telegram', 'click', 'practice', 'exam']\n",
        "                    found_keywords = [kw for kw in keywords if kw.lower() in text_content.lower()]\n",
        "\n",
        "                    if found_keywords:\n",
        "                        print(f\"   ‚Ä¢ Content type indicators: {', '.join(found_keywords)}\")\n",
        "\n",
        "                # Quick line detection test\n",
        "                print(f\"\\nüìè Line Detection Test:\")\n",
        "                line_test = splitter.detect_vertical_lines(pdf_file, sample_pages=3)\n",
        "\n",
        "                print(f\"   ‚Ä¢ Lines detected: {len(line_test.get('detected_lines', []))}\")\n",
        "                print(f\"   ‚Ä¢ Suggested split: {line_test.get('optimal_split', 0.5):.1%}\")\n",
        "                print(f\"   ‚Ä¢ Confidence: {line_test.get('confidence', 0):.1%}\")\n",
        "\n",
        "                if line_test.get('confidence', 0) > 0.7:\n",
        "                    print(\"   ‚úÖ Excellent line detection\")\n",
        "                elif line_test.get('confidence', 0) > 0.4:\n",
        "                    print(\"   ‚ö†Ô∏è Moderate line detection\")\n",
        "                else:\n",
        "                    print(\"   ‚ùå Poor line detection - may need manual ratio\")\n",
        "\n",
        "                # Content structure analysis\n",
        "                print(f\"\\nüß† Content Structure:\")\n",
        "\n",
        "                blocks = first_page.get_text(\"dict\").get(\"blocks\", [])\n",
        "                text_blocks = [b for b in blocks if \"lines\" in b]\n",
        "\n",
        "                print(f\"   ‚Ä¢ Text blocks: {len(text_blocks)}\")\n",
        "\n",
        "                if text_blocks:\n",
        "                    x_positions = []\n",
        "                    for block in text_blocks:\n",
        "                        bbox = block[\"bbox\"]\n",
        "                        x_positions.extend([bbox[0], bbox[2]])\n",
        "\n",
        "                    x_positions.sort()\n",
        "                    page_width = first_page.rect.width\n",
        "\n",
        "                    # Check distribution\n",
        "                    left_content = sum(1 for x in x_positions if x < page_width * 0.5)\n",
        "                    right_content = sum(1 for x in x_positions if x >= page_width * 0.5)\n",
        "\n",
        "                    print(f\"   ‚Ä¢ Content distribution: {left_content} left, {right_content} right\")\n",
        "\n",
        "                    if abs(left_content - right_content) > 5:\n",
        "                        print(\"   ‚úÖ Uneven distribution - good for splitting\")\n",
        "                    else:\n",
        "                        print(\"   ‚ö†Ô∏è Even distribution - splitting may not be beneficial\")\n",
        "\n",
        "                # Recommendations\n",
        "                print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "\n",
        "                confidence = line_test.get('confidence', 0)\n",
        "                split_ratio = line_test.get('optimal_split', 0.5)\n",
        "\n",
        "                if confidence > 0.7:\n",
        "                    print(f\"   üü¢ PROCEED: Use automatic detection ({split_ratio:.1%})\")\n",
        "                elif confidence > 0.4:\n",
        "                    print(f\"   üü° CAUTION: Review suggested split ({split_ratio:.1%})\")\n",
        "                else:\n",
        "                    print(f\"   üî¥ MANUAL: Consider manual inspection\")\n",
        "\n",
        "                # Check for potential issues\n",
        "                issues = []\n",
        "\n",
        "                if file_size > 100:\n",
        "                    issues.append(\"Very large file - processing may be slow\")\n",
        "\n",
        "                if len(doc) > 200:\n",
        "                    issues.append(\"Many pages - consider batch processing\")\n",
        "\n",
        "                if split_ratio < 0.2 or split_ratio > 0.8:\n",
        "                    issues.append(f\"Unusual split ratio ({split_ratio:.1%})\")\n",
        "\n",
        "                if len(text_content) < 100:\n",
        "                    issues.append(\"Very little text - might be image-based PDF\")\n",
        "\n",
        "                if issues:\n",
        "                    print(f\"\\n‚ö†Ô∏è POTENTIAL ISSUES:\")\n",
        "                    for issue in issues:\n",
        "                        print(f\"   ‚Ä¢ {issue}\")\n",
        "                else:\n",
        "                    print(f\"\\n‚úÖ No issues detected - PDF looks good for processing!\")\n",
        "\n",
        "            finally:\n",
        "                doc.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Diagnostic error: {e}\")\n",
        "\n",
        "def show_help_and_usage():\n",
        "    \"\"\"Show comprehensive help and usage guide\"\"\"\n",
        "\n",
        "    print(\"üìö COMPLETE PDF SPLITTER - HELP & USAGE GUIDE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"üïê Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ User: Ravi-katta-dev\")\n",
        "    print(f\"üì± Session: {datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\"\"\n",
        "üéØ AVAILABLE FUNCTIONS:\n",
        "\n",
        "1. üöÄ QUICK PROCESSING:\n",
        "   quick_split_now()                    - Fast split with smart defaults\n",
        "\n",
        "2. üéÆ INTERACTIVE MODE:\n",
        "   run_interactive_mode()               - Step-by-step guided processing\n",
        "\n",
        "3. üß† COMPREHENSIVE ANALYSIS:\n",
        "   test_comprehensive_analysis()        - Test all detection methods\n",
        "   run_intelligent_split()              - Full multi-method processing\n",
        "\n",
        "4. üì¶ BATCH PROCESSING:\n",
        "   batch_process_pdfs()                 - Process multiple PDFs\n",
        "\n",
        "5. üîç DIAGNOSTIC TOOLS:\n",
        "   diagnose_pdf_issues()                - Analyze PDF structure and issues\n",
        "\n",
        "6. ‚ÑπÔ∏è HELP & INFO:\n",
        "   show_help_and_usage()                - This help guide\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üéØ RECOMMENDED WORKFLOW:\n",
        "\n",
        "For First-Time Users:\n",
        "1. Upload your PDF file\n",
        "2. Run: diagnose_pdf_issues()          # Check for any issues\n",
        "3. Run: test_comprehensive_analysis()   # See what methods work best\n",
        "4. Run: run_interactive_mode()         # Process with guidance\n",
        "\n",
        "For Quick Processing:\n",
        "1. Upload your PDF file\n",
        "2. Run: quick_split_now()              # Fast processing\n",
        "\n",
        "For Multiple Files:\n",
        "1. Upload multiple PDF files\n",
        "2. Run: batch_process_pdfs()           # Process all at once\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üîß DETECTION METHODS:\n",
        "\n",
        "üìè Line Detection:\n",
        "   - Finds actual vertical lines in your PDF\n",
        "   - Perfect for documents with visible dividers\n",
        "   - High accuracy for structured documents\n",
        "\n",
        "üß† Content Analysis:\n",
        "   - Analyzes text density and distribution\n",
        "   - Good for documents without visible lines\n",
        "   - Works with natural content boundaries\n",
        "\n",
        "üëÅÔ∏è Visual Pattern Recognition:\n",
        "   - Recognizes visual layout patterns\n",
        "   - Detects column structures and alignments\n",
        "   - Useful for complex layouts\n",
        "\n",
        "üìã Document Structure Analysis:\n",
        "   - Understands document type and purpose\n",
        "   - Optimizes for exam papers, forms, etc.\n",
        "   - Context-aware processing\n",
        "\n",
        "üéØ Multi-Method Fusion:\n",
        "   - Combines all methods for best accuracy\n",
        "   - Highest confidence results\n",
        "   - Recommended for unknown document types\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üí° TROUBLESHOOTING:\n",
        "\n",
        "‚ùå \"No PDF files found\":\n",
        "   - Upload a PDF file first\n",
        "   - Check file has .pdf extension\n",
        "   - Restart runtime if needed\n",
        "\n",
        "‚ùå \"Processing failed\":\n",
        "   - Run diagnose_pdf_issues() to check PDF\n",
        "   - Try different detection method\n",
        "   - Check if PDF is password protected\n",
        "\n",
        "‚ùå \"Download failed\":\n",
        "   - File is still processed locally\n",
        "   - Try downloading manually\n",
        "   - Check internet connection\n",
        "\n",
        "‚ùå \"Low confidence detection\":\n",
        "   - PDF might have unusual layout\n",
        "   - Try manual ratio adjustment\n",
        "   - Use visual inspection\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üéì TIPS FOR BEST RESULTS:\n",
        "\n",
        "‚úÖ PDF Quality:\n",
        "   - Use high-quality, text-based PDFs\n",
        "   - Avoid heavily compressed files\n",
        "   - Ensure text is selectable\n",
        "\n",
        "‚úÖ Document Type:\n",
        "   - Works best with structured documents\n",
        "   - Exam papers, forms, and reports ideal\n",
        "   - Two-column layouts process excellently\n",
        "\n",
        "‚úÖ File Size:\n",
        "   - Files under 50MB process fastest\n",
        "   - Large files may take longer\n",
        "   - Consider batch processing for multiple files\n",
        "\n",
        "‚úÖ Split Ratios:\n",
        "   - 40-60% typically work best\n",
        "   - Extreme ratios (20% or 80%) may indicate issues\n",
        "   - Trust high-confidence detections\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "üÜò SUPPORT:\n",
        "\n",
        "If you encounter issues:\n",
        "1. Run diagnose_pdf_issues() first\n",
        "2. Check the troubleshooting section above\n",
        "3. Try different processing methods\n",
        "4. Consider manual ratio adjustment\n",
        "\n",
        "For best results with your exam papers:\n",
        "- Use quick_split_now() for simple cases\n",
        "- Use run_interactive_mode() for guidance\n",
        "- Use test_comprehensive_analysis() to verify detection\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# üöÄ SYSTEM INITIALIZATION AND STATUS\n",
        "# =============================================================================\n",
        "\n",
        "def system_status():\n",
        "    \"\"\"Show current system status and available files\"\"\"\n",
        "\n",
        "    print(\"üìä SYSTEM STATUS REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üïê Current Time: 2025-08-08 12:22:52 UTC\")\n",
        "    print(f\"üë§ Current User: Ravi-katta-dev\")\n",
        "    print(f\"üêç Python Environment: Google Colab\")\n",
        "    print(f\"üì¶ PyMuPDF Version: {fitz.version[0] if hasattr(fitz, 'version') else 'Unknown'}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check current directory contents\n",
        "    all_files = os.listdir('.')\n",
        "    pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "    print(f\"üìÅ Current Directory Analysis:\")\n",
        "    print(f\"   ‚Ä¢ Total files: {len(all_files)}\")\n",
        "    print(f\"   ‚Ä¢ PDF files: {len(pdf_files)}\")\n",
        "\n",
        "    if pdf_files:\n",
        "        print(f\"\\nüìã PDF Files Found:\")\n",
        "\n",
        "        input_pdfs = []\n",
        "        output_pdfs = []\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            file_size = os.path.getsize(pdf) / (1024 * 1024)\n",
        "\n",
        "            if pdf.startswith(('SMART_', 'LINE_', 'PRECISION_', 'INTELLIGENT_', 'COMPREHENSIVE_', 'CUSTOM_', 'QUICK_', 'BATCH_')):\n",
        "                output_pdfs.append((pdf, file_size))\n",
        "            else:\n",
        "                input_pdfs.append((pdf, file_size))\n",
        "\n",
        "        if input_pdfs:\n",
        "            print(f\"\\nüì• Input PDFs ({len(input_pdfs)}):\")\n",
        "            for pdf, size in input_pdfs:\n",
        "                print(f\"   üìÑ {pdf} ({size:.2f} MB)\")\n",
        "\n",
        "        if output_pdfs:\n",
        "            print(f\"\\nüì§ Processed PDFs ({len(output_pdfs)}):\")\n",
        "            for pdf, size in output_pdfs:\n",
        "                print(f\"   üìÑ {pdf} ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå No PDF files found\")\n",
        "        print(\"üí° Upload a PDF file to get started\")\n",
        "\n",
        "    # Memory status\n",
        "    print(f\"\\nüíæ Memory Status:\")\n",
        "    try:\n",
        "        import psutil\n",
        "        memory = psutil.virtual_memory()\n",
        "        print(f\"   ‚Ä¢ Available: {memory.available / (1024**3):.1f} GB\")\n",
        "        print(f\"   ‚Ä¢ Usage: {memory.percent:.1f}%\")\n",
        "    except:\n",
        "        print(\"   ‚Ä¢ Memory info not available\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"   üî∏ Upload a PDF file first\")\n",
        "        print(\"   üî∏ Run show_help_and_usage() for guidance\")\n",
        "    elif input_pdfs and not output_pdfs:\n",
        "        print(\"   üî∏ Ready to process! Try quick_split_now()\")\n",
        "        print(\"   üî∏ Or run run_interactive_mode() for guided processing\")\n",
        "    elif input_pdfs and output_pdfs:\n",
        "        print(\"   üî∏ Some files already processed\")\n",
        "        print(\"   üî∏ Check output files or process remaining inputs\")\n",
        "    else:\n",
        "        print(\"   üî∏ Only output files found\")\n",
        "        print(\"   üî∏ Upload new input PDFs or download existing outputs\")\n",
        "\n",
        "# Initialize the complete system\n",
        "print(\"üéØ COMPLETE PDF SPLITTER v4.0 - FULLY LOADED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üïê System Time: 2025-08-08 12:22:52 UTC\")\n",
        "print(f\"üë§ Current User: Ravi-katta-dev\")\n",
        "print(f\"üöÄ Status: All systems operational\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Show system status\n",
        "system_status()\n",
        "\n",
        "print(f\"\\nüéÆ QUICK START COMMANDS:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"üöÄ quick_split_now()              # Fast processing\")\n",
        "print(\"üéÆ run_interactive_mode()         # Step-by-step guidance\")\n",
        "print(\"üß™ test_comprehensive_analysis()  # Test all methods\")\n",
        "print(\"üîç diagnose_pdf_issues()          # Check PDF health\")\n",
        "print(\"üìö show_help_and_usage()          # Complete guide\")\n",
        "print(\"üìä system_status()                # System information\")\n",
        "\n",
        "print(f\"\\n‚ú® Ready for processing! Choose a command above to get started.\")"
      ],
      "metadata": {
        "id": "6d6MNMTqpUAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa29d19-9b8e-4e14-9be3-6e2b1e2d8134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful!\n",
            "üïê Session started: 2025-08-08 12:18:45 UTC\n",
            "üë§ User: Ravi-katta-dev\n",
            "üéØ COMPLETE PDF SPLITTER v4.0 - FULLY LOADED!\n",
            "======================================================================\n",
            "üïê System Time: 2025-08-08 12:22:52 UTC\n",
            "üë§ Current User: Ravi-katta-dev\n",
            "üöÄ Status: All systems operational\n",
            "======================================================================\n",
            "üìä SYSTEM STATUS REPORT\n",
            "==================================================\n",
            "üïê Current Time: 2025-08-08 12:22:52 UTC\n",
            "üë§ Current User: Ravi-katta-dev\n",
            "üêç Python Environment: Google Colab\n",
            "üì¶ PyMuPDF Version: 1.26.3\n",
            "==================================================\n",
            "üìÅ Current Directory Analysis:\n",
            "   ‚Ä¢ Total files: 2\n",
            "   ‚Ä¢ PDF files: 0\n",
            "\n",
            "‚ùå No PDF files found\n",
            "üí° Upload a PDF file to get started\n",
            "\n",
            "üíæ Memory Status:\n",
            "   ‚Ä¢ Available: 11.4 GB\n",
            "   ‚Ä¢ Usage: 9.9%\n",
            "\n",
            "üí° RECOMMENDATIONS:\n",
            "   üî∏ Upload a PDF file first\n",
            "   üî∏ Run show_help_and_usage() for guidance\n",
            "\n",
            "üéÆ QUICK START COMMANDS:\n",
            "------------------------------\n",
            "üöÄ quick_split_now()              # Fast processing\n",
            "üéÆ run_interactive_mode()         # Step-by-step guidance\n",
            "üß™ test_comprehensive_analysis()  # Test all methods\n",
            "üîç diagnose_pdf_issues()          # Check PDF health\n",
            "üìö show_help_and_usage()          # Complete guide\n",
            "üìä system_status()                # System information\n",
            "\n",
            "‚ú® Ready for processing! Choose a command above to get started.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_help_and_usage()"
      ],
      "metadata": {
        "id": "m0cm7ghenR0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8dd358-9f41-4d1d-940f-09731f4d039d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö COMPLETE PDF SPLITTER - HELP & USAGE GUIDE\n",
            "======================================================================\n",
            "üïê Time: 2025-08-08 12:22:52 UTC\n",
            "üë§ User: Ravi-katta-dev\n",
            "üì± Session: 20250808_124149\n",
            "======================================================================\n",
            "\n",
            "üéØ AVAILABLE FUNCTIONS:\n",
            "\n",
            "1. üöÄ QUICK PROCESSING:\n",
            "   quick_split_now()                    - Fast split with smart defaults\n",
            "   \n",
            "2. üéÆ INTERACTIVE MODE:\n",
            "   run_interactive_mode()               - Step-by-step guided processing\n",
            "   \n",
            "3. üß† COMPREHENSIVE ANALYSIS:\n",
            "   test_comprehensive_analysis()        - Test all detection methods\n",
            "   run_intelligent_split()              - Full multi-method processing\n",
            "   \n",
            "4. üì¶ BATCH PROCESSING:\n",
            "   batch_process_pdfs()                 - Process multiple PDFs\n",
            "   \n",
            "5. üîç DIAGNOSTIC TOOLS:\n",
            "   diagnose_pdf_issues()                - Analyze PDF structure and issues\n",
            "   \n",
            "6. ‚ÑπÔ∏è HELP & INFO:\n",
            "   show_help_and_usage()                - This help guide\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üéØ RECOMMENDED WORKFLOW:\n",
            "\n",
            "For First-Time Users:\n",
            "1. Upload your PDF file\n",
            "2. Run: diagnose_pdf_issues()          # Check for any issues\n",
            "3. Run: test_comprehensive_analysis()   # See what methods work best\n",
            "4. Run: run_interactive_mode()         # Process with guidance\n",
            "\n",
            "For Quick Processing:\n",
            "1. Upload your PDF file\n",
            "2. Run: quick_split_now()              # Fast processing\n",
            "\n",
            "For Multiple Files:\n",
            "1. Upload multiple PDF files\n",
            "2. Run: batch_process_pdfs()           # Process all at once\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üîß DETECTION METHODS:\n",
            "\n",
            "üìè Line Detection:\n",
            "   - Finds actual vertical lines in your PDF\n",
            "   - Perfect for documents with visible dividers\n",
            "   - High accuracy for structured documents\n",
            "\n",
            "üß† Content Analysis:\n",
            "   - Analyzes text density and distribution\n",
            "   - Good for documents without visible lines\n",
            "   - Works with natural content boundaries\n",
            "\n",
            "üëÅÔ∏è Visual Pattern Recognition:\n",
            "   - Recognizes visual layout patterns\n",
            "   - Detects column structures and alignments\n",
            "   - Useful for complex layouts\n",
            "\n",
            "üìã Document Structure Analysis:\n",
            "   - Understands document type and purpose\n",
            "   - Optimizes for exam papers, forms, etc.\n",
            "   - Context-aware processing\n",
            "\n",
            "üéØ Multi-Method Fusion:\n",
            "   - Combines all methods for best accuracy\n",
            "   - Highest confidence results\n",
            "   - Recommended for unknown document types\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üí° TROUBLESHOOTING:\n",
            "\n",
            "‚ùå \"No PDF files found\":\n",
            "   - Upload a PDF file first\n",
            "   - Check file has .pdf extension\n",
            "   - Restart runtime if needed\n",
            "\n",
            "‚ùå \"Processing failed\":\n",
            "   - Run diagnose_pdf_issues() to check PDF\n",
            "   - Try different detection method\n",
            "   - Check if PDF is password protected\n",
            "\n",
            "‚ùå \"Download failed\":\n",
            "   - File is still processed locally\n",
            "   - Try downloading manually\n",
            "   - Check internet connection\n",
            "\n",
            "‚ùå \"Low confidence detection\":\n",
            "   - PDF might have unusual layout\n",
            "   - Try manual ratio adjustment\n",
            "   - Use visual inspection\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üéì TIPS FOR BEST RESULTS:\n",
            "\n",
            "‚úÖ PDF Quality:\n",
            "   - Use high-quality, text-based PDFs\n",
            "   - Avoid heavily compressed files\n",
            "   - Ensure text is selectable\n",
            "\n",
            "‚úÖ Document Type:\n",
            "   - Works best with structured documents\n",
            "   - Exam papers, forms, and reports ideal\n",
            "   - Two-column layouts process excellently\n",
            "\n",
            "‚úÖ File Size:\n",
            "   - Files under 50MB process fastest\n",
            "   - Large files may take longer\n",
            "   - Consider batch processing for multiple files\n",
            "\n",
            "‚úÖ Split Ratios:\n",
            "   - 40-60% typically work best\n",
            "   - Extreme ratios (20% or 80%) may indicate issues\n",
            "   - Trust high-confidence detections\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "üÜò SUPPORT:\n",
            "\n",
            "If you encounter issues:\n",
            "1. Run diagnose_pdf_issues() first\n",
            "2. Check the troubleshooting section above\n",
            "3. Try different processing methods\n",
            "4. Consider manual ratio adjustment\n",
            "\n",
            "For best results with your exam papers:\n",
            "- Use quick_split_now() for simple cases\n",
            "- Use run_interactive_mode() for guidance\n",
            "- Use test_comprehensive_analysis() to verify detection\n",
            "\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_interactive_mode()"
      ],
      "metadata": {
        "id": "ax4ER7QCDtUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_comprehensive_analysis()"
      ],
      "metadata": {
        "id": "7H8AVLzvD1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559,
          "referenced_widgets": [
            "e4e19417abd74540b2a2adfad7f8f734",
            "810418fd60a24b6ba145acf4c587a88e",
            "06f086c2109b4a91bea3bd7a27eefdb3",
            "54286564b8dc4f22919b0c3b67ff939b",
            "fb94dfc626834c4daa6e3a759b5c3365",
            "ea954cb62a5c4cf99139e2d184c9e77d",
            "a1d3aad9cefc4627b347c64fd7288101",
            "22e6927063304d18b0004bbc0cd52da7",
            "347a560f43874bcd9baecd398bc85fd3",
            "b2c5ab019c9d4cb6aa7375b2adf4942e",
            "2c7527778b6f4f7db88e71b1599b6b29",
            "a15801ac8b644a0d8a4eb55e286bcd86",
            "adf739016cd543419c2f8520a153f124",
            "af477ceb27a4454b852e1a35f75ec8cb",
            "1283aa92f1174212b6f6f659bb9fb56b",
            "e965b2f2a87d4fea9377d5b1f89721d7",
            "dec8175247c34d799f583523406514a2"
          ]
        },
        "id": "vqCRFuY6nEFu",
        "outputId": "c3c720d9-db59-4942-eee0-e1be9bbeb381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ INTELLIGENT PDF SPLITTER v3.0\n",
            "============================================================\n",
            "üß† AI-Powered Content-Aware PDF Splitting for Google Colab\n",
            "============================================================\n",
            "üéØ Intelligent PDF Splitter initialized!\n",
            "üéØ Intelligent PDF Splitter Configuration\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h3>üéØ Configuration Options</h3>'), Dropdown(description='Split Mode:', options=(('‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4e19417abd74540b2a2adfad7f8f734"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÅ Upload your PDF files:\n",
            "üí° The AI will analyze content layout automatically\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-919675cf-a607-4474-be95-39994ab3e233\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-919675cf-a607-4474-be95-39994ab3e233\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "main_intelligent()  # Interactive mode with UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vpz_0sRhoP1o",
        "outputId": "008d9014-cc34-4344-db39-7a951ad0d269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß QUICK FIX MODE\n",
            "==============================\n",
            "üìÅ Found existing PDF files: ['SPLIT_Tech Practice1-80.pdf', 'Tech Practice1-80.pdf']\n",
            "üì• Attempting to download: SPLIT_Tech Practice1-80.pdf\n",
            "üì• Downloading: SPLIT_Tech Practice1-80.pdf (attempt 1)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_5fa8041e-7bbe-44a4-b777-ea0f60a3b208\", \"SPLIT_Tech Practice1-80.pdf\", 3588133)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Download successful!\n"
          ]
        }
      ],
      "source": [
        "quick_fix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5QfZvx0mnnI",
        "outputId": "a72b37dd-ce3e-49da-bf1a-39c225d0fb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Setting up Enhanced PDF Splitter...\n",
            "==================================================\n",
            "\n",
            "üì¶ Installing dependencies...\n",
            "üì¶ Installing pymupdf...\n",
            "‚úÖ pymupdf installed successfully!\n",
            "üì¶ Installing Pillow...\n",
            "‚úÖ Pillow installed successfully!\n",
            "‚úÖ tqdm already installed\n",
            "\n",
            "‚úÖ All dependencies installed successfully!\n",
            "üéâ Ready to load the PDF Splitter!\n",
            "\n",
            "==================================================\n",
            "Next: Run the second cell to load the PDF Splitter code\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# üöÄ ENHANCED PDF SPLITTER - GOOGLE COLAB SETUP\n",
        "# =============================================================================\n",
        "# Run this cell first to install all dependencies and set up the environment\n",
        "\n",
        "print(\"üîß Setting up Enhanced PDF Splitter...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package with progress indication\"\"\"\n",
        "    try:\n",
        "        __import__(package.replace('-', '_').split('[')[0])\n",
        "        print(f\"‚úÖ {package} already installed\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"üì¶ Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        print(f\"‚úÖ {package} installed successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Required packages\n",
        "packages = [\n",
        "    \"pymupdf\",      # PDF processing\n",
        "    \"Pillow\",       # Image processing\n",
        "    \"tqdm\",         # Progress bars\n",
        "]\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "all_installed = True\n",
        "for package in packages:\n",
        "    if not install_package(package):\n",
        "        all_installed = False\n",
        "\n",
        "if all_installed:\n",
        "    print(\"\\n‚úÖ All dependencies installed successfully!\")\n",
        "    print(\"üéâ Ready to load the PDF Splitter!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Some packages failed to install. Please try restarting the runtime.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Next: Run the second cell to load the PDF Splitter code\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quick_split_now()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949,
          "referenced_widgets": [
            "5451c7d284734fa4ab2e944729622134",
            "00fec06b70fe4bb3ae452e7f3cbf949b",
            "b6e8b20605f14e40bddc74e750f8c8b6",
            "0bb7cca078de471eb83689069ee7c2f5",
            "2bc369cddae44763bfc9826b067a1bb7",
            "4e07a56bf48a4cc89fe59d64fb5e517e",
            "801ab21ac196495ca6c938035b3abed4",
            "3bc50e72a4d7429798098150b1dc068f",
            "fb627f370a2d4998b6a8ad72fcdfbb7d",
            "bc5e3d6cf70444309afd84cc1e272f66",
            "bd4a0c95b93244f791c905e92c85f297"
          ]
        },
        "id": "I_zRv7SUqRR0",
        "outputId": "972e34fe-546a-4655-894d-1ce5ae0aa871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° QUICK SPLIT - FAST PROCESSING\n",
            "==================================================\n",
            "üïê Time: 2025-08-08 12:22:52 UTC\n",
            "üë§ User: Ravi-katta-dev\n",
            "‚ö° Fast processing with smart defaults\n",
            "==================================================\n",
            "üìÅ Processing: Tech Practice1-80.pdf\n",
            "üìä File size: 3.53 MB\n",
            "üéØ Complete PDF Splitter v4.0 initialized!\n",
            "üì± Session ID: 20250808_124712\n",
            "üöÄ Quick processing...\n",
            "üîç Step 1: Comprehensive PDF Analysis\n",
            "==================================================\n",
            "üéØ Running comprehensive multi-method analysis...\n",
            "üìè Method 1: Vertical line detection...\n",
            "üìè Analyzing 10 pages for vertical lines...\n",
            "üß† Method 2: Content layout analysis...\n",
            "üß† Analyzing content layout from 5 pages...\n",
            "üëÅÔ∏è Method 3: Visual pattern recognition...\n",
            "üìã Method 4: Document structure analysis...\n",
            "üîÑ Combining all analysis methods...\n",
            "\n",
            "üìä Analysis Results:\n",
            "   ‚Ä¢ Detection method: multi_method_combined\n",
            "   ‚Ä¢ Optimal split ratio: 54.3%\n",
            "   ‚Ä¢ Detection confidence: 78.4%\n",
            "   ‚Ä¢ Line detection confidence: 86.0%\n",
            "   ‚Ä¢ Content analysis confidence: 50.0%\n",
            "   ‚Ä¢ Methods agreement: 99.3%\n",
            "\n",
            "‚úÇÔ∏è Step 2: Applying Split at 54.3%\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "üìÑ Processing:   0%|          | 0/80 [00:00<?, ?page/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5451c7d284734fa4ab2e944729622134"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Step 3: Saving Results\n",
            "==================================================\n",
            "‚úÖ Processing Complete!\n",
            "\n",
            "üìä Results Summary:\n",
            "   ‚Ä¢ Input file: Tech Practice1-80.pdf\n",
            "   ‚Ä¢ Output file: QUICK_SPLIT_Tech Practice1-80_124712.pdf\n",
            "   ‚Ä¢ Pages processed: 80/80\n",
            "   ‚Ä¢ Pages created: 160\n",
            "   ‚Ä¢ Output size: 3.42 MB\n",
            "   ‚Ä¢ Processing time: 1.08 seconds\n",
            "   ‚Ä¢ Speed: 74.2 pages/second\n",
            "   ‚Ä¢ Split ratio used: 54.3%\n",
            "   ‚Ä¢ Detection confidence: 78.4%\n",
            "   ‚Ä¢ Success rate: 100.0%\n",
            "‚úÖ Done! Downloading QUICK_SPLIT_Tech Practice1-80_124712.pdf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_05ec4c83-4759-4d46-a09d-1abf98b350e0\", \"QUICK_SPLIT_Tech Practice1-80_124712.pdf\", 3589733)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Success! Your split PDF has been downloaded.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdMzEycI7m3Cx9/VI32nTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4e19417abd74540b2a2adfad7f8f734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_810418fd60a24b6ba145acf4c587a88e",
              "IPY_MODEL_06f086c2109b4a91bea3bd7a27eefdb3",
              "IPY_MODEL_54286564b8dc4f22919b0c3b67ff939b",
              "IPY_MODEL_fb94dfc626834c4daa6e3a759b5c3365",
              "IPY_MODEL_ea954cb62a5c4cf99139e2d184c9e77d"
            ],
            "layout": "IPY_MODEL_a1d3aad9cefc4627b347c64fd7288101"
          }
        },
        "810418fd60a24b6ba145acf4c587a88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22e6927063304d18b0004bbc0cd52da7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_347a560f43874bcd9baecd398bc85fd3",
            "value": "<h3>üéØ Configuration Options</h3>"
          }
        },
        "06f086c2109b4a91bea3bd7a27eefdb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "ü§ñ Intelligent Auto-Split (Recommended)",
              "üìÑ Vertical Split (Left/Right)",
              "üìÑ Horizontal Split (Top/Bottom)",
              "üéõÔ∏è Custom Vertical Split",
              "üéõÔ∏è Custom Horizontal Split"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Split Mode:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_b2c5ab019c9d4cb6aa7375b2adf4942e",
            "style": "IPY_MODEL_2c7527778b6f4f7db88e71b1599b6b29"
          }
        },
        "54286564b8dc4f22919b0c3b67ff939b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FloatSliderView",
            "continuous_update": true,
            "description": "Custom Ratio:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a15801ac8b644a0d8a4eb55e286bcd86",
            "max": 0.9,
            "min": 0.1,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": ".0%",
            "step": 0.05,
            "style": "IPY_MODEL_adf739016cd543419c2f8520a153f124",
            "value": 0.5
          }
        },
        "fb94dfc626834c4daa6e3a759b5c3365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Show detailed content analysis",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_af477ceb27a4454b852e1a35f75ec8cb",
            "style": "IPY_MODEL_1283aa92f1174212b6f6f659bb9fb56b",
            "value": true
          }
        },
        "ea954cb62a5c4cf99139e2d184c9e77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e965b2f2a87d4fea9377d5b1f89721d7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dec8175247c34d799f583523406514a2",
            "value": "\n        <div style='background-color: #e8f4fd; padding: 10px; border-radius: 5px; margin: 10px 0;'>\n            <b>ü§ñ Intelligent Mode Features:</b><br>\n            ‚Ä¢ Analyzes text layout and content distribution<br>\n            ‚Ä¢ Finds natural split points in content<br>\n            ‚Ä¢ Avoids cutting through text blocks<br>\n            ‚Ä¢ Optimizes for content preservation<br>\n            ‚Ä¢ Works great for exam papers, documents, and forms\n        </div>\n        "
          }
        },
        "a1d3aad9cefc4627b347c64fd7288101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e6927063304d18b0004bbc0cd52da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347a560f43874bcd9baecd398bc85fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2c5ab019c9d4cb6aa7375b2adf4942e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7527778b6f4f7db88e71b1599b6b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "120px"
          }
        },
        "a15801ac8b644a0d8a4eb55e286bcd86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "none",
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf739016cd543419c2f8520a153f124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "120px",
            "handle_color": null
          }
        },
        "af477ceb27a4454b852e1a35f75ec8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1283aa92f1174212b6f6f659bb9fb56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "e965b2f2a87d4fea9377d5b1f89721d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec8175247c34d799f583523406514a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5451c7d284734fa4ab2e944729622134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00fec06b70fe4bb3ae452e7f3cbf949b",
              "IPY_MODEL_b6e8b20605f14e40bddc74e750f8c8b6",
              "IPY_MODEL_0bb7cca078de471eb83689069ee7c2f5"
            ],
            "layout": "IPY_MODEL_2bc369cddae44763bfc9826b067a1bb7"
          }
        },
        "00fec06b70fe4bb3ae452e7f3cbf949b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e07a56bf48a4cc89fe59d64fb5e517e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_801ab21ac196495ca6c938035b3abed4",
            "value": "üìÑ‚ÄáProcessing:‚Äá100%"
          }
        },
        "b6e8b20605f14e40bddc74e750f8c8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc50e72a4d7429798098150b1dc068f",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb627f370a2d4998b6a8ad72fcdfbb7d",
            "value": 80
          }
        },
        "0bb7cca078de471eb83689069ee7c2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc5e3d6cf70444309afd84cc1e272f66",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd4a0c95b93244f791c905e92c85f297",
            "value": "‚Äá80/80‚Äá[00:00&lt;00:00,‚Äá127.54page/s,‚ÄáSplit=54.3%,‚ÄáSuccess=80/80]"
          }
        },
        "2bc369cddae44763bfc9826b067a1bb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e07a56bf48a4cc89fe59d64fb5e517e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801ab21ac196495ca6c938035b3abed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc50e72a4d7429798098150b1dc068f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb627f370a2d4998b6a8ad72fcdfbb7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc5e3d6cf70444309afd84cc1e272f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4a0c95b93244f791c905e92c85f297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}